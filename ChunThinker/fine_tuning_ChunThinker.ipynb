{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUaRQpC3IRhu"
      },
      "source": [
        "### Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c5-UgqZ9IRhu"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Normally using pip install unsloth is enough\n",
        "\n",
        "# Temporarily as of Jan 31st 2025, Colab has some issues with Pytorch\n",
        "# Using pip install unsloth will take 3 minutes, whilst the below takes <1 minute:\n",
        "%pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "%pip install --no-deps \"xformers<0.0.29\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "%pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "%pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "\n",
        "%pip install -U bitsandbytes\n",
        "%pip install git+https://github.com/PanQiWei/AutoGPTQ.git@v0.4.2 \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvnFV5Q2IRhv"
      },
      "source": [
        "### Unsloth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "bwgZ9UPFIRhv",
        "outputId": "bdf1e71f-f6a2-4b0c-9a9a-2c361bf55fb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nguyentrung/Documents/project/fine_tuning_model/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.49.0.\n",
            "   \\\\   /|    GPU: NVIDIA GeForce RTX 4060. Max memory: 7.739 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = (\n",
        "    None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        ")\n",
        "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",  # Llama-3.1 2x faster\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",  # 4bit for 405b!\n",
        "    \"unsloth/Mistral-Small-Instruct-2409\",  # Mistral 22b 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",  # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",  # Gemma 2x faster!\n",
        "    \"unsloth/Llama-3.2-1B-bnb-4bit\",  # NEW! Llama 3.2 models\n",
        "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen2.5-7B\",\n",
        "    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\",  # NEW! Llama 3.3 70B!\n",
        "]  # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3.2-3B\",  # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "    use_gradient_checkpointing=True,  # Ti·∫øt ki·ªám VRAM\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "6181aa51-47aa-4ff0-ff13-e5e993a7c647"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.2.12 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
        "    bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
        "    random_state=3407,\n",
        "    use_rslora=False,  # We support rank stabilized LoRA\n",
        "    loftq_config=None,  # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c7e834a42885469cb4e291a0c78ede14",
            "14fcca8d922a450c8afcab53e5aa65e1",
            "796191af9a0d4bab9377d13a05da4b5f",
            "e1d893dc93374ff389b9283019d3bbea",
            "248d471ae26842e7947a1c4087f8097e",
            "14c6d25d2c6a433ca772fbf8c95061d8",
            "8b6a38b01cd44c73859da2e459e8a80f",
            "a7780eaba753460eb5d03a48cf0c371a",
            "070ca8d4788040439d70b289123ed7f5",
            "bb2e1e90765e4ce1a02cf7e8a9f6d881",
            "e84444abd7e843328a762cade219777d"
          ]
        },
        "id": "LjY75GoYUCB8",
        "outputId": "188f7433-3457-4843-fb53-c27b52ab7bb4"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template, standardize_sharegpt\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "\n",
        "system_message =\"\"\"\n",
        "    B·∫°n l√† chunGPT ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi ƒë·∫°i ca V∆∞∆°ng Nguy√™n Trung.\n",
        "    Your role as an assistant involves thoroughly exploring questions through a systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution. In the Thought section, detail your reasoning process using the specified format:\n",
        "    <reasoning>\n",
        "        {reasoning with steps separated with '\\n\\n'}\n",
        "    </reasoning>\n",
        "    <answer>\n",
        "        {final formatted, precise, and clear solution}\n",
        "    </answer>\n",
        "\"\"\"\n",
        "# First load tokenizer and dataset\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"llama-3.1\",\n",
        "    # chat_template=\"chatml\",\n",
        "    # system_message=system_message,\n",
        ")\n",
        "\n",
        "dataset = load_dataset(\"1TuanPham/Vietnamese-OpenO1-SFT\", split=\"vi\")\n",
        "# en_dataset = load_dataset(\"1TuanPham/Vietnamese-OpenO1-SFT\", split=\"en\")\n",
        "\n",
        "# K·∫øt h·ª£p v√† x·ª≠ l√Ω ƒë·ªãnh d·∫°ng\n",
        "# dataset = concatenate_datasets([vi_dataset, en_dataset])\n",
        "# (T√πy ch·ªçn) L·ªçc c√°c c·ªôt kh√¥ng c·∫ßn thi·∫øt n·∫øu mu·ªën\n",
        "dataset = dataset.remove_columns([\"qas_id\"])  # Gi·ªØ l·∫°i n·∫øu c·∫ßn ID\n",
        "\n",
        "\n",
        "def add_conversations_column(examples):\n",
        "    conversations = [\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_message,\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"{instr}\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": f\"<reasoning>{cot}</reasoning>\\n\\n<answer>{out}</answer>\",\n",
        "            },\n",
        "        ]\n",
        "        for instr, cot, out in zip(\n",
        "            examples[\"instruction\"], examples[\"cot\"], examples[\"output\"]\n",
        "        )\n",
        "    ]\n",
        "    return {\"conversations\": conversations}\n",
        "\n",
        "\n",
        "dataset = dataset.map(add_conversations_column, batched=True)\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = []\n",
        "    for convo in convos:\n",
        "        # Th√™m c·ªù truncation v√† max_length\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            convo,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "            truncation=True,\n",
        "            max_length=2048,\n",
        "        )\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 July 2024\n",
            "\n",
            "Test system<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Test instruction<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Test response<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "# Ki·ªÉm tra template m·∫´u\n",
        "test_convo = [\n",
        "    {\"role\": \"system\", \"content\": \"Test system\"},\n",
        "    {\"role\": \"user\", \"content\": \"Test instruction\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Test response\"},\n",
        "]\n",
        "print(tokenizer.apply_chat_template(test_convo, tokenize=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "vhXv0xFMGNKE",
        "outputId": "efe51dee-3cd1-45bb-d407-bb656f1b3224"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n\\n    B·∫°n l√† chunGPT ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi ƒë·∫°i ca V∆∞∆°ng Nguy√™n Trung.\\n    Your role as an assistant involves thoroughly exploring questions through a systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution. In the Thought section, detail your reasoning process using the specified format:\\n    <reasoning>\\n        {reasoning with steps separated with \\'\\n\\n\\'}\\n    </reasoning>\\n    <answer>\\n        {final formatted, precise, and clear solution}\\n    </answer>\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nVi·∫øt cho t√¥i m·ªôt quy tr√¨nh l√†m vi·ªác AWS step function ·ªü ƒë·ªãnh d·∫°ng json, quy tr√¨nh n√†y ƒë·∫ßu ti√™n chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu JSON b·∫±ng c√°ch s·ª≠ d·ª•ng m·ªôt m·∫´u √°nh x·∫° tr∆∞·ªõc khi chuy·ªÉn n√≥ ƒë·∫øn m·ªôt h√†m lambda ƒë·ªÉ x·ª≠ l√Ω<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n<reasoning>ƒê∆∞·ª£c r·ªìi, t√¥i c·∫ßn t·∫°o m·ªôt quy tr√¨nh l√†m vi·ªác AWS Step Functions b·∫±ng JSON ƒë·ªÉ th·ª±c hi·ªán hai t√°c v·ª• ch√≠nh: ƒë·∫ßu ti√™n, n√≥ chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu JSON b·∫±ng c√°ch s·ª≠ d·ª•ng m·ªôt m·∫´u √°nh x·∫°, v√† sau ƒë√≥ n√≥ chuy·ªÉn d·ªØ li·ªáu ƒë√£ chuy·ªÉn ƒë·ªïi ƒë·∫øn m·ªôt h√†m Lambda ƒë·ªÉ x·ª≠ l√Ω th√™m. H√£y ƒë·ªÉ t√¥i chia nh·ªè ƒëi·ªÅu n√†y t·ª´ng b∆∞·ªõc ƒë·ªÉ ƒë·∫£m b·∫£o t√¥i bao g·ªìm t·∫•t c·∫£ c√°c th√†nh ph·∫ßn c·∫ßn thi·∫øt.\\n\\n**B∆∞·ªõc 1: Hi·ªÉu c√°c Y√™u c·∫ßu**\\n- **D·ªØ li·ªáu ƒê·∫ßu v√†o**: Quy tr√¨nh l√†m vi·ªác s·∫Ω nh·∫≠n d·ªØ li·ªáu JSON c·∫ßn ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi.\\n- **Chuy·ªÉn ƒë·ªïi**: S·ª≠ d·ª•ng m·ªôt m·∫´u √°nh x·∫° ƒë·ªÉ chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu JSON. ƒêi·ªÅu n√†y c√≥ th·ªÉ li√™n quan ƒë·∫øn vi·ªác t√°i c·∫•u tr√∫c ho·∫∑c ƒë·ªãnh d·∫°ng d·ªØ li·ªáu theo y√™u c·∫ßu c·ªßa qu√° tr√¨nh x·ª≠ l√Ω ti·∫øp theo.\\n- **X·ª≠ l√Ω**: Sau khi chuy·ªÉn ƒë·ªïi, d·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c chuy·ªÉn ƒë·∫øn m·ªôt h√†m Lambda ƒë·ªÉ x·ª≠ l√Ω. ƒêi·ªÅu n√†y c√≥ th·ªÉ li√™n quan ƒë·∫øn c√°c t√≠nh to√°n, x√°c th·ª±c ho·∫∑c b·∫•t k·ª≥ logic nghi·ªáp v·ª• n√†o.\\n\\n**B∆∞·ªõc 2: X√°c ƒë·ªãnh C·∫•u tr√∫c Quy tr√¨nh l√†m vi·ªác**\\nAWS Step Functions s·ª≠ d·ª•ng m·ªôt m√°y tr·∫°ng th√°i ƒë·ªÉ x√°c ƒë·ªãnh quy tr√¨nh l√†m vi·ªác. Quy tr√¨nh l√†m vi·ªác s·∫Ω c√≥ c√°c tr·∫°ng th√°i sau:\\n1. **Tr·∫°ng th√°i B·∫Øt ƒë·∫ßu**: Kh·ªüi t·∫°o quy tr√¨nh l√†m vi·ªác.\\n2. **Tr·∫°ng th√°i Chuy·ªÉn ƒë·ªïi D·ªØ li·ªáu**: √Åp d·ª•ng m·∫´u √°nh x·∫° cho d·ªØ li·ªáu JSON ƒë·∫ßu v√†o.\\n3. **Tr·∫°ng th√°i X·ª≠ l√Ω D·ªØ li·ªáu**: G·ªçi h√†m Lambda v·ªõi d·ªØ li·ªáu ƒë√£ chuy·ªÉn ƒë·ªïi.\\n4. **Tr·∫°ng th√°i K·∫øt th√∫c**: Ho√†n th√†nh quy tr√¨nh l√†m vi·ªác.\\n\\n**B∆∞·ªõc 3: Ch·ªçn ƒê√∫ng Lo·∫°i Tr·∫°ng th√°i**\\n- **Tr·∫°ng th√°i Chuy·ªÉn ƒë·ªïi D·ªØ li·ªáu**: ƒê√¢y c√≥ th·ªÉ l√† tr·∫°ng th√°i **Pass** v·ªõi **ResultPath** √°p d·ª•ng m·∫´u √°nh x·∫°.\\n- **Tr·∫°ng th√°i X·ª≠ l√Ω D·ªØ li·ªáu**: ƒê√¢y s·∫Ω l√† tr·∫°ng th√°i **Task** g·ªçi h√†m Lambda.\\n- **Tr·∫°ng th√°i B·∫Øt ƒë·∫ßu v√† K·∫øt th√∫c**: ƒê√¢y l√† c√°c tr·∫°ng th√°i ti√™u chu·∫©n ƒë·ªÉ b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c quy tr√¨nh l√†m vi·ªác.\\n\\n**B∆∞·ªõc 4: Vi·∫øt M·∫´u √Ånh x·∫°**\\nM·∫´u √°nh x·∫° s·∫Ω x√°c ƒë·ªãnh c√°ch d·ªØ li·ªáu JSON ƒë·∫ßu v√†o ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi. V√≠ d·ª•, n√≥ c√≥ th·ªÉ tr√≠ch xu·∫•t c√°c tr∆∞·ªùng c·ª• th·ªÉ, ƒë·ªïi t√™n ch√∫ng ho·∫∑c t√°i c·∫•u tr√∫c h·ªá th·ªëng ph√¢n c·∫•p d·ªØ li·ªáu. M·∫´u c√≥ th·ªÉ ƒë∆∞·ª£c vi·∫øt b·∫±ng JSON ho·∫∑c m·ªôt ƒë·ªãnh d·∫°ng ƒë∆∞·ª£c h·ªó tr·ª£ kh√°c nh∆∞ JSONPath.\\n\\n**B∆∞·ªõc 5: C·∫•u h√¨nh H√†m Lambda**\\nƒê·∫£m b·∫£o r·∫±ng h√†m Lambda ƒë∆∞·ª£c c·∫•u h√¨nh ƒë√∫ng ƒë·ªÉ nh·∫≠n d·ªØ li·ªáu ƒë√£ chuy·ªÉn ƒë·ªïi. ƒêi·ªÅu n√†y bao g·ªìm vi·ªác thi·∫øt l·∫≠p ƒë√∫ng vai tr√≤ v√† quy·ªÅn IAM ƒë·ªÉ Step Functions c√≥ th·ªÉ g·ªçi h√†m Lambda.\\n\\n**B∆∞·ªõc 6: X·ª≠ l√Ω L·ªói v√† Th·ª≠ l·∫°i**\\nƒêi·ªÅu quan tr·ªçng l√† ph·∫£i bao g·ªìm x·ª≠ l√Ω l·ªói ƒë·ªÉ qu·∫£n l√Ω c√°c l·ªói trong qu√° tr√¨nh chuy·ªÉn ƒë·ªïi ho·∫∑c x·ª≠ l√Ω. ƒêi·ªÅu n√†y c√≥ th·ªÉ li√™n quan ƒë·∫øn c√°c c∆° ch·∫ø th·ª≠ l·∫°i ho·∫∑c c√°c tr·∫°ng th√°i d·ª± ph√≤ng.\\n\\n**B∆∞·ªõc 7: X√°c ƒë·ªãnh M√°y Tr·∫°ng th√°i trong JSON**\\nB√¢y gi·ªù, t√¥i s·∫Ω c·∫•u tr√∫c m√°y tr·∫°ng th√°i trong JSON, k·∫øt h·ª£p t·∫•t c·∫£ c√°c y·∫øu t·ªë tr√™n. T√¥i s·∫Ω ƒë·∫£m b·∫£o r·∫±ng m·ªói tr·∫°ng th√°i ƒë∆∞·ª£c x√°c ƒë·ªãnh ch√≠nh x√°c v·ªõi lo·∫°i c·ªßa n√≥, tr·∫°ng th√°i ti·∫øp theo v√† b·∫•t k·ª≥ tham s·ªë c·∫ßn thi·∫øt n√†o nh∆∞ m·∫´u √°nh x·∫° v√† ARN h√†m Lambda.\\n\\n**B∆∞·ªõc 8: Xem x√©t v√† X√°c th·ª±c**\\nSau khi ph√°c th·∫£o JSON, t√¥i s·∫Ω xem x√©t n√≥ ƒë·ªÉ ƒë·∫£m b·∫£o t·∫•t c·∫£ c√∫ ph√°p ƒë·ªÅu ch√≠nh x√°c, c√°c tr·∫°ng th√°i ƒë∆∞·ª£c li√™n k·∫øt ƒë√∫ng c√°ch v√† quy tr√¨nh l√†m vi·ªác logic t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi. T√¥i c≈©ng s·∫Ω x√°c th·ª±c r·∫±ng h√†m Lambda ƒë∆∞·ª£c tham chi·∫øu ch√≠nh x√°c v√† m·∫´u √°nh x·∫° chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu ch√≠nh x√°c nh∆∞ d·ª± ƒë·ªãnh.\\n\\n**C√°c Th√°ch th·ª©c v√† C√¢n nh·∫Øc Ti·ªÅm nƒÉng**\\n- **X√°c th·ª±c D·ªØ li·ªáu**: ƒê·∫£m b·∫£o r·∫±ng d·ªØ li·ªáu ƒë·∫ßu v√†o tu√¢n th·ªß ƒë·ªãnh d·∫°ng d·ª± ki·∫øn ƒë·ªÉ ngƒÉn ng·ª´a l·ªói trong qu√° tr√¨nh chuy·ªÉn ƒë·ªïi.\\n- **Gi·ªõi h·∫°n H√†m Lambda**: L∆∞u √Ω ƒë·∫øn th·ªùi gian th·ª±c thi v√† gi·ªõi h·∫°n b·ªô nh·ªõ c·ªßa h√†m Lambda ƒë·ªÉ x·ª≠ l√Ω hi·ªáu qu·∫£.\\n- **X·ª≠ l√Ω L·ªói**: Tri·ªÉn khai x·ª≠ l√Ω l·ªói m·∫°nh m·∫Ω ƒë·ªÉ qu·∫£n l√Ω m·ªôt c√°ch duy√™n d√°ng b·∫•t k·ª≥ l·ªói n√†o m√† kh√¥ng l√†m cho to√†n b·ªô quy tr√¨nh l√†m vi·ªác b·ªã l·ªói b·∫•t ng·ªù.\\n- **B·∫£o m·∫≠t**: ƒê·∫£m b·∫£o r·∫±ng Step Functions v√† h√†m Lambda c√≥ c√°c ƒë·∫∑c quy·ªÅn t·ªëi thi·ªÉu c·∫ßn thi·∫øt ƒë·ªÉ th·ª±c hi·ªán c√°c t√°c v·ª• c·ªßa ch√∫ng, tu√¢n th·ªß nguy√™n t·∫Øc ƒë·∫∑c quy·ªÅn t·ªëi thi·ªÉu.\\n\\nB·∫±ng c√°ch l·∫≠p k·∫ø ho·∫°ch t·ªâ m·ªâ cho t·ª´ng th√†nh ph·∫ßn v√† d·ª± ƒëo√°n c√°c v·∫•n ƒë·ªÅ ti·ªÅm ·∫©n, t√¥i c√≥ th·ªÉ t·∫°o ra m·ªôt quy tr√¨nh l√†m vi·ªác AWS Step Functions ƒë√°ng tin c·∫≠y v√† hi·ªáu qu·∫£ ƒë√°p ·ª©ng c√°c y√™u c·∫ßu.</reasoning>\\n\\n<answer>{\\n  \"StartAt\": \"Transform Data\",\\n  \"States\": {\\n    \"Transform Data\": {\\n      \"Type\": \"Pass\",\\n      \"Result\": {\\n        \"inputData\": {\\n          \"name\": \"$.input.name\",\\n          \"age\": \"$.input.age\",\\n          \"email\": \"$.input.email\"\\n        }\\n      },\\n      \"ResultPath\": \"$.transformedData\",\\n      \"Next\": \"Process Data\"\\n    },\\n    \"Process Data\": {\\n      \"Type\": \"Task\",\\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:ProcessDataFunction\",\\n      \"InputPath\": \"$.transformedData\",\\n      \"ResultPath\": \"$.processedData\",\\n      \"Next\": \"End State\"\\n    },\\n    \"End State\": {\\n      \"Type\": \"Succeed\"\\n    }\\n  }\\n}</answer><|eot_id|>'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[5][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMfTfBC-VeK2",
        "outputId": "14cb8f79-6c5b-4c94-c6c6-531941552a1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'content': \"\\n    B·∫°n l√† chunGPT ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi ƒë·∫°i ca V∆∞∆°ng Nguy√™n Trung.\\n    Your role as an assistant involves thoroughly exploring questions through a systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution. In the Thought section, detail your reasoning process using the specified format:\\n    <reasoning>\\n        {reasoning with steps separated with '\\n\\n'}\\n    </reasoning>\\n    <answer>\\n        {final formatted, precise, and clear solution}\\n    </answer>\\n\",\n",
              "  'role': 'system'},\n",
              " {'content': 'Vi·∫øt cho t√¥i m·ªôt quy tr√¨nh l√†m vi·ªác AWS step function ·ªü ƒë·ªãnh d·∫°ng json, quy tr√¨nh n√†y ƒë·∫ßu ti√™n chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu JSON b·∫±ng c√°ch s·ª≠ d·ª•ng m·ªôt m·∫´u √°nh x·∫° tr∆∞·ªõc khi chuy·ªÉn n√≥ ƒë·∫øn m·ªôt h√†m lambda ƒë·ªÉ x·ª≠ l√Ω',\n",
              "  'role': 'user'},\n",
              " {'content': '<reasoning>ƒê∆∞·ª£c r·ªìi, t√¥i c·∫ßn t·∫°o m·ªôt quy tr√¨nh l√†m vi·ªác AWS Step Functions b·∫±ng JSON ƒë·ªÉ th·ª±c hi·ªán hai t√°c v·ª• ch√≠nh: ƒë·∫ßu ti√™n, n√≥ chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu JSON b·∫±ng c√°ch s·ª≠ d·ª•ng m·ªôt m·∫´u √°nh x·∫°, v√† sau ƒë√≥ n√≥ chuy·ªÉn d·ªØ li·ªáu ƒë√£ chuy·ªÉn ƒë·ªïi ƒë·∫øn m·ªôt h√†m Lambda ƒë·ªÉ x·ª≠ l√Ω th√™m. H√£y ƒë·ªÉ t√¥i chia nh·ªè ƒëi·ªÅu n√†y t·ª´ng b∆∞·ªõc ƒë·ªÉ ƒë·∫£m b·∫£o t√¥i bao g·ªìm t·∫•t c·∫£ c√°c th√†nh ph·∫ßn c·∫ßn thi·∫øt.\\n\\n**B∆∞·ªõc 1: Hi·ªÉu c√°c Y√™u c·∫ßu**\\n- **D·ªØ li·ªáu ƒê·∫ßu v√†o**: Quy tr√¨nh l√†m vi·ªác s·∫Ω nh·∫≠n d·ªØ li·ªáu JSON c·∫ßn ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi.\\n- **Chuy·ªÉn ƒë·ªïi**: S·ª≠ d·ª•ng m·ªôt m·∫´u √°nh x·∫° ƒë·ªÉ chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu JSON. ƒêi·ªÅu n√†y c√≥ th·ªÉ li√™n quan ƒë·∫øn vi·ªác t√°i c·∫•u tr√∫c ho·∫∑c ƒë·ªãnh d·∫°ng d·ªØ li·ªáu theo y√™u c·∫ßu c·ªßa qu√° tr√¨nh x·ª≠ l√Ω ti·∫øp theo.\\n- **X·ª≠ l√Ω**: Sau khi chuy·ªÉn ƒë·ªïi, d·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c chuy·ªÉn ƒë·∫øn m·ªôt h√†m Lambda ƒë·ªÉ x·ª≠ l√Ω. ƒêi·ªÅu n√†y c√≥ th·ªÉ li√™n quan ƒë·∫øn c√°c t√≠nh to√°n, x√°c th·ª±c ho·∫∑c b·∫•t k·ª≥ logic nghi·ªáp v·ª• n√†o.\\n\\n**B∆∞·ªõc 2: X√°c ƒë·ªãnh C·∫•u tr√∫c Quy tr√¨nh l√†m vi·ªác**\\nAWS Step Functions s·ª≠ d·ª•ng m·ªôt m√°y tr·∫°ng th√°i ƒë·ªÉ x√°c ƒë·ªãnh quy tr√¨nh l√†m vi·ªác. Quy tr√¨nh l√†m vi·ªác s·∫Ω c√≥ c√°c tr·∫°ng th√°i sau:\\n1. **Tr·∫°ng th√°i B·∫Øt ƒë·∫ßu**: Kh·ªüi t·∫°o quy tr√¨nh l√†m vi·ªác.\\n2. **Tr·∫°ng th√°i Chuy·ªÉn ƒë·ªïi D·ªØ li·ªáu**: √Åp d·ª•ng m·∫´u √°nh x·∫° cho d·ªØ li·ªáu JSON ƒë·∫ßu v√†o.\\n3. **Tr·∫°ng th√°i X·ª≠ l√Ω D·ªØ li·ªáu**: G·ªçi h√†m Lambda v·ªõi d·ªØ li·ªáu ƒë√£ chuy·ªÉn ƒë·ªïi.\\n4. **Tr·∫°ng th√°i K·∫øt th√∫c**: Ho√†n th√†nh quy tr√¨nh l√†m vi·ªác.\\n\\n**B∆∞·ªõc 3: Ch·ªçn ƒê√∫ng Lo·∫°i Tr·∫°ng th√°i**\\n- **Tr·∫°ng th√°i Chuy·ªÉn ƒë·ªïi D·ªØ li·ªáu**: ƒê√¢y c√≥ th·ªÉ l√† tr·∫°ng th√°i **Pass** v·ªõi **ResultPath** √°p d·ª•ng m·∫´u √°nh x·∫°.\\n- **Tr·∫°ng th√°i X·ª≠ l√Ω D·ªØ li·ªáu**: ƒê√¢y s·∫Ω l√† tr·∫°ng th√°i **Task** g·ªçi h√†m Lambda.\\n- **Tr·∫°ng th√°i B·∫Øt ƒë·∫ßu v√† K·∫øt th√∫c**: ƒê√¢y l√† c√°c tr·∫°ng th√°i ti√™u chu·∫©n ƒë·ªÉ b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c quy tr√¨nh l√†m vi·ªác.\\n\\n**B∆∞·ªõc 4: Vi·∫øt M·∫´u √Ånh x·∫°**\\nM·∫´u √°nh x·∫° s·∫Ω x√°c ƒë·ªãnh c√°ch d·ªØ li·ªáu JSON ƒë·∫ßu v√†o ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi. V√≠ d·ª•, n√≥ c√≥ th·ªÉ tr√≠ch xu·∫•t c√°c tr∆∞·ªùng c·ª• th·ªÉ, ƒë·ªïi t√™n ch√∫ng ho·∫∑c t√°i c·∫•u tr√∫c h·ªá th·ªëng ph√¢n c·∫•p d·ªØ li·ªáu. M·∫´u c√≥ th·ªÉ ƒë∆∞·ª£c vi·∫øt b·∫±ng JSON ho·∫∑c m·ªôt ƒë·ªãnh d·∫°ng ƒë∆∞·ª£c h·ªó tr·ª£ kh√°c nh∆∞ JSONPath.\\n\\n**B∆∞·ªõc 5: C·∫•u h√¨nh H√†m Lambda**\\nƒê·∫£m b·∫£o r·∫±ng h√†m Lambda ƒë∆∞·ª£c c·∫•u h√¨nh ƒë√∫ng ƒë·ªÉ nh·∫≠n d·ªØ li·ªáu ƒë√£ chuy·ªÉn ƒë·ªïi. ƒêi·ªÅu n√†y bao g·ªìm vi·ªác thi·∫øt l·∫≠p ƒë√∫ng vai tr√≤ v√† quy·ªÅn IAM ƒë·ªÉ Step Functions c√≥ th·ªÉ g·ªçi h√†m Lambda.\\n\\n**B∆∞·ªõc 6: X·ª≠ l√Ω L·ªói v√† Th·ª≠ l·∫°i**\\nƒêi·ªÅu quan tr·ªçng l√† ph·∫£i bao g·ªìm x·ª≠ l√Ω l·ªói ƒë·ªÉ qu·∫£n l√Ω c√°c l·ªói trong qu√° tr√¨nh chuy·ªÉn ƒë·ªïi ho·∫∑c x·ª≠ l√Ω. ƒêi·ªÅu n√†y c√≥ th·ªÉ li√™n quan ƒë·∫øn c√°c c∆° ch·∫ø th·ª≠ l·∫°i ho·∫∑c c√°c tr·∫°ng th√°i d·ª± ph√≤ng.\\n\\n**B∆∞·ªõc 7: X√°c ƒë·ªãnh M√°y Tr·∫°ng th√°i trong JSON**\\nB√¢y gi·ªù, t√¥i s·∫Ω c·∫•u tr√∫c m√°y tr·∫°ng th√°i trong JSON, k·∫øt h·ª£p t·∫•t c·∫£ c√°c y·∫øu t·ªë tr√™n. T√¥i s·∫Ω ƒë·∫£m b·∫£o r·∫±ng m·ªói tr·∫°ng th√°i ƒë∆∞·ª£c x√°c ƒë·ªãnh ch√≠nh x√°c v·ªõi lo·∫°i c·ªßa n√≥, tr·∫°ng th√°i ti·∫øp theo v√† b·∫•t k·ª≥ tham s·ªë c·∫ßn thi·∫øt n√†o nh∆∞ m·∫´u √°nh x·∫° v√† ARN h√†m Lambda.\\n\\n**B∆∞·ªõc 8: Xem x√©t v√† X√°c th·ª±c**\\nSau khi ph√°c th·∫£o JSON, t√¥i s·∫Ω xem x√©t n√≥ ƒë·ªÉ ƒë·∫£m b·∫£o t·∫•t c·∫£ c√∫ ph√°p ƒë·ªÅu ch√≠nh x√°c, c√°c tr·∫°ng th√°i ƒë∆∞·ª£c li√™n k·∫øt ƒë√∫ng c√°ch v√† quy tr√¨nh l√†m vi·ªác logic t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi. T√¥i c≈©ng s·∫Ω x√°c th·ª±c r·∫±ng h√†m Lambda ƒë∆∞·ª£c tham chi·∫øu ch√≠nh x√°c v√† m·∫´u √°nh x·∫° chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu ch√≠nh x√°c nh∆∞ d·ª± ƒë·ªãnh.\\n\\n**C√°c Th√°ch th·ª©c v√† C√¢n nh·∫Øc Ti·ªÅm nƒÉng**\\n- **X√°c th·ª±c D·ªØ li·ªáu**: ƒê·∫£m b·∫£o r·∫±ng d·ªØ li·ªáu ƒë·∫ßu v√†o tu√¢n th·ªß ƒë·ªãnh d·∫°ng d·ª± ki·∫øn ƒë·ªÉ ngƒÉn ng·ª´a l·ªói trong qu√° tr√¨nh chuy·ªÉn ƒë·ªïi.\\n- **Gi·ªõi h·∫°n H√†m Lambda**: L∆∞u √Ω ƒë·∫øn th·ªùi gian th·ª±c thi v√† gi·ªõi h·∫°n b·ªô nh·ªõ c·ªßa h√†m Lambda ƒë·ªÉ x·ª≠ l√Ω hi·ªáu qu·∫£.\\n- **X·ª≠ l√Ω L·ªói**: Tri·ªÉn khai x·ª≠ l√Ω l·ªói m·∫°nh m·∫Ω ƒë·ªÉ qu·∫£n l√Ω m·ªôt c√°ch duy√™n d√°ng b·∫•t k·ª≥ l·ªói n√†o m√† kh√¥ng l√†m cho to√†n b·ªô quy tr√¨nh l√†m vi·ªác b·ªã l·ªói b·∫•t ng·ªù.\\n- **B·∫£o m·∫≠t**: ƒê·∫£m b·∫£o r·∫±ng Step Functions v√† h√†m Lambda c√≥ c√°c ƒë·∫∑c quy·ªÅn t·ªëi thi·ªÉu c·∫ßn thi·∫øt ƒë·ªÉ th·ª±c hi·ªán c√°c t√°c v·ª• c·ªßa ch√∫ng, tu√¢n th·ªß nguy√™n t·∫Øc ƒë·∫∑c quy·ªÅn t·ªëi thi·ªÉu.\\n\\nB·∫±ng c√°ch l·∫≠p k·∫ø ho·∫°ch t·ªâ m·ªâ cho t·ª´ng th√†nh ph·∫ßn v√† d·ª± ƒëo√°n c√°c v·∫•n ƒë·ªÅ ti·ªÅm ·∫©n, t√¥i c√≥ th·ªÉ t·∫°o ra m·ªôt quy tr√¨nh l√†m vi·ªác AWS Step Functions ƒë√°ng tin c·∫≠y v√† hi·ªáu qu·∫£ ƒë√°p ·ª©ng c√°c y√™u c·∫ßu.</reasoning>\\n\\n<answer>{\\n  \"StartAt\": \"Transform Data\",\\n  \"States\": {\\n    \"Transform Data\": {\\n      \"Type\": \"Pass\",\\n      \"Result\": {\\n        \"inputData\": {\\n          \"name\": \"$.input.name\",\\n          \"age\": \"$.input.age\",\\n          \"email\": \"$.input.email\"\\n        }\\n      },\\n      \"ResultPath\": \"$.transformedData\",\\n      \"Next\": \"Process Data\"\\n    },\\n    \"Process Data\": {\\n      \"Type\": \"Task\",\\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:ProcessDataFunction\",\\n      \"InputPath\": \"$.transformedData\",\\n      \"ResultPath\": \"$.processedData\",\\n      \"Next\": \"End State\"\\n    },\\n    \"End State\": {\\n      \"Type\": \"Succeed\"\\n    }\\n  }\\n}</answer>',\n",
              "  'role': 'assistant'}]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[5][\"conversations\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "\n",
        "### Train the model\n",
        "\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128,
          "referenced_widgets": [
            "f66ceb5f96fe4ae38193a4a6f9a4fd60",
            "c2c710d06dd04428a8595296c52828bb",
            "8e4c415dfe09453bbe50816fc815b1a2",
            "4d14e81bc0c642ad833e61f764f521e3",
            "6e5ed953d2a741cbb9ba6d954a1e7c18",
            "12184c89275c42a59f3c5f7a512cf8e6",
            "c65192e89e6046299174394d62c31002",
            "40edbd257909455b95f13c75ccec8661",
            "a0072ad5352348c3a551f1c6818445d4",
            "ee420abea7814a7c874cc4d30d53778f",
            "34c9e9cfbf104c90a8d0b8ec30021edd",
            "326c8c02704742aba6eb9906b6ab565d",
            "7720d0627c7e4770bb8417ca3d53e82c",
            "05361a49bec247a989b92d0273f8a8a6",
            "fda2b821efb347258145ac593cf49c0d",
            "a830b7f0feda4748978b95024204e1bc",
            "9884af3cc32647acaf3b9de913d4d21d",
            "51fc4cde067746cda57fbc2b64e02d40",
            "261f2d39bc1247b1b7ec557c92626f44",
            "5a8544ba4489494097d446606349043a",
            "9a211854308d4cf896022aead86a6fc6",
            "dbd1c96a6c4a4b3a81457436407e75ec",
            "a3ec2615a9d343329f17a4a2278a8a14",
            "3cc9bdfb85704b17a64f7097ebf4a44b",
            "e3a9a42cae3d419e95ffedc046a9887c",
            "29f24afedecc43db95ac4e7ab1fa01e0",
            "be00b0e96faf46098160b2dd33b448cc",
            "9705c35042184d2db14ebae9d9f1ecfa",
            "270314604ce74721b857c5204d41b283",
            "d664254d90224b0d9738d7fc92dac1c9",
            "cd1085f167374f8f96a4fd61feb125c9",
            "3dd1efc8384f4356ade13143f9774bdd",
            "8dc92c23eff34236aedc83724db607b7"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "419440c2-f11f-4b7b-d03b-005fc1bbb974"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,  # Can make training 5x faster for short sequences.\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=3,\n",
        "        warmup_steps=100,\n",
        "        num_train_epochs=2,  # Set this for 1 full training run.\n",
        "        # max_steps=300,\n",
        "        # save_steps=250,\n",
        "        learning_rate=5e-6,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=50,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",  # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_sGp5XlG6dq"
      },
      "source": [
        "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510,
          "referenced_widgets": [
            "7f69c2f0ad70491b82ed1b550c2741b1",
            "467ba48099b0486798322ceb4b206bf5",
            "3a7731db739e4674a93db274fb135d7e",
            "1a92e911567146e5908e7e680ecd4905",
            "c9f860dd5ef24a42988a1adb79a1a30b",
            "df7d41a027c8446ebb11bd0210c5c103",
            "c484ba18d3134eb8815498b2328717bb",
            "3db45cd840274a8f8b4835d4b594839d",
            "a5bc57b3afb94473a994077567332651",
            "37a5403876f8466ba0cf676ca1a3f345",
            "b8cb050625fe44f59cc75fb4669ee287"
          ]
        },
        "id": "juQiExuBG5Bt",
        "outputId": "3e481109-e247-47c6-b6c7-d5b8e5b47f6d"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "\n",
        "if True:\n",
        "    trainer = train_on_responses_only(\n",
        "        trainer,\n",
        "        instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "        response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        "    )\n",
        "\n",
        "if False:\n",
        "    trainer = train_on_responses_only(\n",
        "        trainer,\n",
        "        instruction_part=\"<|im_start|>user\\n\",  # V√≠ d·ª• cho ChatML\n",
        "        response_part=\"<|im_start|>assistant\\n\",  # V√≠ d·ª• cho ChatML\n",
        "        # Ho·∫∑c cho Llama 3:\n",
        "        # instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "        # response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv1NBUozV78l"
      },
      "source": [
        "We verify masking is actually done:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "LtsMVtlkUhja",
        "outputId": "63eccda8-3084-409f-ae06-3308b9c63504"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n\\n    B·∫°n l√† chunGPT ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi ƒë·∫°i ca V∆∞∆°ng Nguy√™n Trung.\\n    Your role as an assistant involves thoroughly exploring questions through a systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution. In the Thought section, detail your reasoning process using the specified format:\\n    <reasoning>\\n        {reasoning with steps separated with \\'\\n\\n\\'}\\n    </reasoning>\\n    <answer>\\n        {final formatted, precise, and clear solution}\\n    </answer>\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nVi·∫øt cho t√¥i m·ªôt quy tr√¨nh l√†m vi·ªác AWS step function ·ªü ƒë·ªãnh d·∫°ng json, quy tr√¨nh n√†y ƒë·∫ßu ti√™n chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu JSON b·∫±ng c√°ch s·ª≠ d·ª•ng m·ªôt m·∫´u √°nh x·∫° tr∆∞·ªõc khi chuy·ªÉn n√≥ ƒë·∫øn m·ªôt h√†m lambda ƒë·ªÉ x·ª≠ l√Ω<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n<reasoning>ƒê∆∞·ª£c r·ªìi, t√¥i c·∫ßn t·∫°o m·ªôt quy tr√¨nh l√†m vi·ªác AWS Step Functions b·∫±ng JSON ƒë·ªÉ th·ª±c hi·ªán hai t√°c v·ª• ch√≠nh: ƒë·∫ßu ti√™n, n√≥ chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu JSON b·∫±ng c√°ch s·ª≠ d·ª•ng m·ªôt m·∫´u √°nh x·∫°, v√† sau ƒë√≥ n√≥ chuy·ªÉn d·ªØ li·ªáu ƒë√£ chuy·ªÉn ƒë·ªïi ƒë·∫øn m·ªôt h√†m Lambda ƒë·ªÉ x·ª≠ l√Ω th√™m. H√£y ƒë·ªÉ t√¥i chia nh·ªè ƒëi·ªÅu n√†y t·ª´ng b∆∞·ªõc ƒë·ªÉ ƒë·∫£m b·∫£o t√¥i bao g·ªìm t·∫•t c·∫£ c√°c th√†nh ph·∫ßn c·∫ßn thi·∫øt.\\n\\n**B∆∞·ªõc 1: Hi·ªÉu c√°c Y√™u c·∫ßu**\\n- **D·ªØ li·ªáu ƒê·∫ßu v√†o**: Quy tr√¨nh l√†m vi·ªác s·∫Ω nh·∫≠n d·ªØ li·ªáu JSON c·∫ßn ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi.\\n- **Chuy·ªÉn ƒë·ªïi**: S·ª≠ d·ª•ng m·ªôt m·∫´u √°nh x·∫° ƒë·ªÉ chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu JSON. ƒêi·ªÅu n√†y c√≥ th·ªÉ li√™n quan ƒë·∫øn vi·ªác t√°i c·∫•u tr√∫c ho·∫∑c ƒë·ªãnh d·∫°ng d·ªØ li·ªáu theo y√™u c·∫ßu c·ªßa qu√° tr√¨nh x·ª≠ l√Ω ti·∫øp theo.\\n- **X·ª≠ l√Ω**: Sau khi chuy·ªÉn ƒë·ªïi, d·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c chuy·ªÉn ƒë·∫øn m·ªôt h√†m Lambda ƒë·ªÉ x·ª≠ l√Ω. ƒêi·ªÅu n√†y c√≥ th·ªÉ li√™n quan ƒë·∫øn c√°c t√≠nh to√°n, x√°c th·ª±c ho·∫∑c b·∫•t k·ª≥ logic nghi·ªáp v·ª• n√†o.\\n\\n**B∆∞·ªõc 2: X√°c ƒë·ªãnh C·∫•u tr√∫c Quy tr√¨nh l√†m vi·ªác**\\nAWS Step Functions s·ª≠ d·ª•ng m·ªôt m√°y tr·∫°ng th√°i ƒë·ªÉ x√°c ƒë·ªãnh quy tr√¨nh l√†m vi·ªác. Quy tr√¨nh l√†m vi·ªác s·∫Ω c√≥ c√°c tr·∫°ng th√°i sau:\\n1. **Tr·∫°ng th√°i B·∫Øt ƒë·∫ßu**: Kh·ªüi t·∫°o quy tr√¨nh l√†m vi·ªác.\\n2. **Tr·∫°ng th√°i Chuy·ªÉn ƒë·ªïi D·ªØ li·ªáu**: √Åp d·ª•ng m·∫´u √°nh x·∫° cho d·ªØ li·ªáu JSON ƒë·∫ßu v√†o.\\n3. **Tr·∫°ng th√°i X·ª≠ l√Ω D·ªØ li·ªáu**: G·ªçi h√†m Lambda v·ªõi d·ªØ li·ªáu ƒë√£ chuy·ªÉn ƒë·ªïi.\\n4. **Tr·∫°ng th√°i K·∫øt th√∫c**: Ho√†n th√†nh quy tr√¨nh l√†m vi·ªác.\\n\\n**B∆∞·ªõc 3: Ch·ªçn ƒê√∫ng Lo·∫°i Tr·∫°ng th√°i**\\n- **Tr·∫°ng th√°i Chuy·ªÉn ƒë·ªïi D·ªØ li·ªáu**: ƒê√¢y c√≥ th·ªÉ l√† tr·∫°ng th√°i **Pass** v·ªõi **ResultPath** √°p d·ª•ng m·∫´u √°nh x·∫°.\\n- **Tr·∫°ng th√°i X·ª≠ l√Ω D·ªØ li·ªáu**: ƒê√¢y s·∫Ω l√† tr·∫°ng th√°i **Task** g·ªçi h√†m Lambda.\\n- **Tr·∫°ng th√°i B·∫Øt ƒë·∫ßu v√† K·∫øt th√∫c**: ƒê√¢y l√† c√°c tr·∫°ng th√°i ti√™u chu·∫©n ƒë·ªÉ b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c quy tr√¨nh l√†m vi·ªác.\\n\\n**B∆∞·ªõc 4: Vi·∫øt M·∫´u √Ånh x·∫°**\\nM·∫´u √°nh x·∫° s·∫Ω x√°c ƒë·ªãnh c√°ch d·ªØ li·ªáu JSON ƒë·∫ßu v√†o ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi. V√≠ d·ª•, n√≥ c√≥ th·ªÉ tr√≠ch xu·∫•t c√°c tr∆∞·ªùng c·ª• th·ªÉ, ƒë·ªïi t√™n ch√∫ng ho·∫∑c t√°i c·∫•u tr√∫c h·ªá th·ªëng ph√¢n c·∫•p d·ªØ li·ªáu. M·∫´u c√≥ th·ªÉ ƒë∆∞·ª£c vi·∫øt b·∫±ng JSON ho·∫∑c m·ªôt ƒë·ªãnh d·∫°ng ƒë∆∞·ª£c h·ªó tr·ª£ kh√°c nh∆∞ JSONPath.\\n\\n**B∆∞·ªõc 5: C·∫•u h√¨nh H√†m Lambda**\\nƒê·∫£m b·∫£o r·∫±ng h√†m Lambda ƒë∆∞·ª£c c·∫•u h√¨nh ƒë√∫ng ƒë·ªÉ nh·∫≠n d·ªØ li·ªáu ƒë√£ chuy·ªÉn ƒë·ªïi. ƒêi·ªÅu n√†y bao g·ªìm vi·ªác thi·∫øt l·∫≠p ƒë√∫ng vai tr√≤ v√† quy·ªÅn IAM ƒë·ªÉ Step Functions c√≥ th·ªÉ g·ªçi h√†m Lambda.\\n\\n**B∆∞·ªõc 6: X·ª≠ l√Ω L·ªói v√† Th·ª≠ l·∫°i**\\nƒêi·ªÅu quan tr·ªçng l√† ph·∫£i bao g·ªìm x·ª≠ l√Ω l·ªói ƒë·ªÉ qu·∫£n l√Ω c√°c l·ªói trong qu√° tr√¨nh chuy·ªÉn ƒë·ªïi ho·∫∑c x·ª≠ l√Ω. ƒêi·ªÅu n√†y c√≥ th·ªÉ li√™n quan ƒë·∫øn c√°c c∆° ch·∫ø th·ª≠ l·∫°i ho·∫∑c c√°c tr·∫°ng th√°i d·ª± ph√≤ng.\\n\\n**B∆∞·ªõc 7: X√°c ƒë·ªãnh M√°y Tr·∫°ng th√°i trong JSON**\\nB√¢y gi·ªù, t√¥i s·∫Ω c·∫•u tr√∫c m√°y tr·∫°ng th√°i trong JSON, k·∫øt h·ª£p t·∫•t c·∫£ c√°c y·∫øu t·ªë tr√™n. T√¥i s·∫Ω ƒë·∫£m b·∫£o r·∫±ng m·ªói tr·∫°ng th√°i ƒë∆∞·ª£c x√°c ƒë·ªãnh ch√≠nh x√°c v·ªõi lo·∫°i c·ªßa n√≥, tr·∫°ng th√°i ti·∫øp theo v√† b·∫•t k·ª≥ tham s·ªë c·∫ßn thi·∫øt n√†o nh∆∞ m·∫´u √°nh x·∫° v√† ARN h√†m Lambda.\\n\\n**B∆∞·ªõc 8: Xem x√©t v√† X√°c th·ª±c**\\nSau khi ph√°c th·∫£o JSON, t√¥i s·∫Ω xem x√©t n√≥ ƒë·ªÉ ƒë·∫£m b·∫£o t·∫•t c·∫£ c√∫ ph√°p ƒë·ªÅu ch√≠nh x√°c, c√°c tr·∫°ng th√°i ƒë∆∞·ª£c li√™n k·∫øt ƒë√∫ng c√°ch v√† quy tr√¨nh l√†m vi·ªác logic t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi. T√¥i c≈©ng s·∫Ω x√°c th·ª±c r·∫±ng h√†m Lambda ƒë∆∞·ª£c tham chi·∫øu ch√≠nh x√°c v√† m·∫´u √°nh x·∫° chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu ch√≠nh x√°c nh∆∞ d·ª± ƒë·ªãnh.\\n\\n**C√°c Th√°ch th·ª©c v√† C√¢n nh·∫Øc Ti·ªÅm nƒÉng**\\n- **X√°c th·ª±c D·ªØ li·ªáu**: ƒê·∫£m b·∫£o r·∫±ng d·ªØ li·ªáu ƒë·∫ßu v√†o tu√¢n th·ªß ƒë·ªãnh d·∫°ng d·ª± ki·∫øn ƒë·ªÉ ngƒÉn ng·ª´a l·ªói trong qu√° tr√¨nh chuy·ªÉn ƒë·ªïi.\\n- **Gi·ªõi h·∫°n H√†m Lambda**: L∆∞u √Ω ƒë·∫øn th·ªùi gian th·ª±c thi v√† gi·ªõi h·∫°n b·ªô nh·ªõ c·ªßa h√†m Lambda ƒë·ªÉ x·ª≠ l√Ω hi·ªáu qu·∫£.\\n- **X·ª≠ l√Ω L·ªói**: Tri·ªÉn khai x·ª≠ l√Ω l·ªói m·∫°nh m·∫Ω ƒë·ªÉ qu·∫£n l√Ω m·ªôt c√°ch duy√™n d√°ng b·∫•t k·ª≥ l·ªói n√†o m√† kh√¥ng l√†m cho to√†n b·ªô quy tr√¨nh l√†m vi·ªác b·ªã l·ªói b·∫•t ng·ªù.\\n- **B·∫£o m·∫≠t**: ƒê·∫£m b·∫£o r·∫±ng Step Functions v√† h√†m Lambda c√≥ c√°c ƒë·∫∑c quy·ªÅn t·ªëi thi·ªÉu c·∫ßn thi·∫øt ƒë·ªÉ th·ª±c hi·ªán c√°c t√°c v·ª• c·ªßa ch√∫ng, tu√¢n th·ªß nguy√™n t·∫Øc ƒë·∫∑c quy·ªÅn t·ªëi thi·ªÉu.\\n\\nB·∫±ng c√°ch l·∫≠p k·∫ø ho·∫°ch t·ªâ m·ªâ cho t·ª´ng th√†nh ph·∫ßn v√† d·ª± ƒëo√°n c√°c v·∫•n ƒë·ªÅ ti·ªÅm ·∫©n, t√¥i c√≥ th·ªÉ t·∫°o ra m·ªôt quy tr√¨nh l√†m vi·ªác AWS Step Functions ƒë√°ng tin c·∫≠y v√† hi·ªáu qu·∫£ ƒë√°p ·ª©ng c√°c y√™u c·∫ßu.</reasoning>\\n\\n<answer>{\\n  \"StartAt\": \"Transform Data\",\\n  \"States\": {\\n    \"Transform Data\": {\\n      \"Type\": \"Pass\",\\n      \"Result\": {\\n        \"inputData\": {\\n          \"name\": \"$.input.name\",\\n          \"age\": \"$.input.age\",\\n          \"email\": \"$.input.email\"\\n        }\\n      },\\n      \"ResultPath\": \"$.transformedData\",\\n      \"Next\": \"Process Data\"\\n    },\\n    \"Process Data\": {\\n      \"Type\": \"Task\",\\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:ProcessDataFunction\",\\n      \"InputPath\": \"$.transformedData\",\\n      \"ResultPath\": \"$.processedData\",\\n      \"Next\": \"End State\"\\n    },\\n    \"End State\": {\\n      \"Type\": \"Succeed\"\\n    }\\n  }\\n}</answer><|eot_id|>'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "_rD6fl8EUxnG",
        "outputId": "073b8bc5-20e2-4eae-cd51-42d84a791a66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'                                                                                                                                                                                                                                                 \\n\\n<reasoning>ƒê∆∞·ª£c th√¥i, t√¥i c·∫ßn t√¨m c√°ch s·ª≠a ƒë·ªïi code hi·ªán c√≥ ƒë·ªÉ qu·∫£n l√Ω m·∫£ng c√°c h√†nh tinh trong H·ªá M·∫∑t Tr·ªùi c·ªßa ch√∫ng ta ƒë·ªÉ th√™m m·ªôt h√†nh tinh m·ªõi. H√†nh tinh m·ªõi ph·∫£i tu√¢n theo quy ∆∞·ªõc ƒë·∫∑t t√™n c·ªßa th·∫ßn tho·∫°i c·ªï ƒëi·ªÉn v√† ph·∫£i ƒë∆∞·ª£c ƒë·ªãnh v·ªã sao cho n√≥ kh√¥ng g·∫ßn M·∫∑t Tr·ªùi h∆°n Sao Th·ªßy ho·∫∑c xa h∆°n Sao Di√™m V∆∞∆°ng. Ch√∫ng ta h√£y chia nh·ªè v·∫•n ƒë·ªÅ n√†y t·ª´ng b∆∞·ªõc.\\n\\nƒê·∫ßu ti√™n, t√¥i n√™n xem x√©t c·∫•u tr√∫c hi·ªán t·∫°i c·ªßa m·∫£ng h√†nh tinh. N√≥ c√≥ kh·∫£ nƒÉng l√† m·ªôt m·∫£ng c√°c ƒë·ªëi t∆∞·ª£ng, trong ƒë√≥ m·ªói ƒë·ªëi t∆∞·ª£ng ƒë·∫°i di·ªán cho m·ªôt h√†nh tinh v·ªõi c√°c thu·ªôc t√≠nh nh∆∞ t√™n, kho·∫£ng c√°ch t·ª´ M·∫∑t Tr·ªùi v√† c√≥ th·ªÉ c√°c thu·ªôc t√≠nh kh√°c. ƒê·ªÉ th√™m m·ªôt h√†nh tinh m·ªõi, t√¥i s·∫Ω c·∫ßn t·∫°o m·ªôt ƒë·ªëi t∆∞·ª£ng m·ªõi v·ªõi c√°c thu·ªôc t√≠nh n√†y.\\n\\nTi·∫øp theo, quy ∆∞·ªõc ƒë·∫∑t t√™n y√™u c·∫ßu t√™n c·ªßa h√†nh tinh m·ªõi ph·∫£i b·∫Øt ngu·ªìn t·ª´ th·∫ßn tho·∫°i c·ªï ƒëi·ªÉn. T√¥i n√™n nghƒ© ƒë·∫øn nh·ªØng c√°i t√™n t·ª´ th·∫ßn tho·∫°i Hy L·∫°p ho·∫∑c La M√£ ch∆∞a ƒë∆∞·ª£c s·ª≠ d·ª•ng b·ªüi c√°c h√†nh tinh hi·ªán c√≥. C√≥ l·∫Ω m·ªôt c√°i g√¨ ƒë√≥ nh∆∞ \"Lysithea\" ho·∫∑c \"Circe\". T√¥i c·∫ßn ƒë·∫£m b·∫£o r·∫±ng t√™n ƒë√£ ch·ªçn ph√π h·ª£p v·ªõi quy t·∫Øc ƒë·∫∑t t√™n hi·ªán c√≥ v√† ch∆∞a ƒë∆∞·ª£c s·ª≠ d·ª•ng.\\n\\nB√¢y gi·ªù, v·ªÅ kho·∫£ng c√°ch t·ª´ M·∫∑t Tr·ªùi, h√†nh tinh m·ªõi kh√¥ng ƒë∆∞·ª£c g·∫ßn h∆°n Sao Th·ªßy ho·∫∑c xa h∆°n Sao Di√™m V∆∞∆°ng. Sao Th·ªßy c√°ch M·∫∑t Tr·ªùi kho·∫£ng 58 tri·ªáu km v√† Sao Di√™m V∆∞∆°ng c√°ch kho·∫£ng 5,9 t·ª∑ km. V√¨ v·∫≠y, kho·∫£ng c√°ch c·ªßa h√†nh tinh m·ªõi ph·∫£i n·∫±m trong ph·∫°m vi n√†y. T√¥i s·∫Ω c·∫ßn quy·∫øt ƒë·ªãnh ch√≠nh x√°c v·ªã tr√≠ ƒë·∫∑t n√≥. C√≥ l·∫Ω ƒë√¢u ƒë√≥ gi·ªØa Tr√°i ƒê·∫•t v√† Sao H·ªèa ƒë·ªÉ duy tr√¨ s·ª± ph√¢n b·ªë c√¢n b·∫±ng.\\n\\nT√¥i c≈©ng n√™n xem x√©t nh·ªØng t√°c ƒë·ªông c·ªßa vi·ªác th√™m m·ªôt h√†nh tinh m·ªõi. ƒêi·ªÅu n√†y c√≥ ·∫£nh h∆∞·ªüng ƒë·∫øn c√°c ph·∫ßn kh√°c c·ªßa code, ch·∫≥ng h·∫°n nh∆∞ c√°c ph√©p t√≠nh li√™n quan ƒë·∫øn chu k·ª≥ qu·ªπ ƒë·∫°o ho·∫∑c t∆∞∆°ng t√°c h·∫•p d·∫´n kh√¥ng? N·∫øu v·∫≠y, t√¥i c≈©ng c√≥ th·ªÉ c·∫ßn c·∫≠p nh·∫≠t c√°c ph·∫ßn ƒë√≥ ƒë·ªÉ t√≠nh ƒë·∫øn s·ª± hi·ªán di·ªán c·ªßa h√†nh tinh m·ªõi.\\n\\nNgo√†i ra, t√¥i c·∫ßn ƒë·∫£m b·∫£o r·∫±ng code v·∫´n hi·ªáu qu·∫£ v√† d·ªÖ b·∫£o tr√¨. C√≥ l·∫Ω vi·ªác tri·ªÉn khai m·ªôt h√†m ƒë·ªÉ th√™m c√°c h√†nh tinh m·ªôt c√°ch linh ho·∫°t s·∫Ω c√≥ l·ª£i, cho ph√©p c√°c l·∫ßn th√™m trong t∆∞∆°ng lai m√† kh√¥ng c·∫ßn s·ª≠a ƒë·ªïi c·∫•u tr√∫c c·ªët l√µi. H√†m n√†y c√≥ th·ªÉ x√°c th·ª±c t√™n c·ªßa h√†nh tinh m·ªõi so v·ªõi c√°c t√™n hi·ªán c√≥ v√† ki·ªÉm tra xem kho·∫£ng c√°ch c·ªßa n√≥ c√≥ n·∫±m trong ph·∫°m vi quy ƒë·ªãnh hay kh√¥ng.\\n\\nX·ª≠ l√Ω l·ªói l√† m·ªôt kh√≠a c·∫°nh kh√°c c·∫ßn xem x√©t. ƒêi·ªÅu g√¨ s·∫Ω x·∫£y ra n·∫øu ai ƒë√≥ c·ªë g·∫Øng th√™m m·ªôt h√†nh tinh c√≥ t√™n ƒë√£ ƒë∆∞·ª£c s·ª≠ d·ª•ng ho·∫∑c c√≥ kho·∫£ng c√°ch kh√¥ng h·ª£p l·ªá? Code n√™n x·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p n√†y m·ªôt c√°ch uy·ªÉn chuy·ªÉn, c√≥ th·ªÉ b·∫±ng c√°ch ƒë∆∞a ra l·ªói ho·∫∑c tr·∫£ v·ªÅ m·ªôt th√¥ng b√°o cho bi·∫øt v·∫•n ƒë·ªÅ.\\n\\nKi·ªÉm th·ª≠ l√† r·∫•t quan tr·ªçng. Sau khi th√™m h√†nh tinh m·ªõi, t√¥i n√™n ch·∫°y th·ª≠ nghi·ªám ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng m·∫£ng ƒë∆∞·ª£c c·∫≠p nh·∫≠t ch√≠nh x√°c v√† t·∫•t c·∫£ c√°c ch·ª©c nƒÉng li√™n quan v·∫´n ho·∫°t ƒë·ªông nh∆∞ mong ƒë·ª£i. ƒêi·ªÅu n√†y bao g·ªìm vi·ªác x√°c minh r·∫±ng c√°c thu·ªôc t√≠nh c·ªßa h√†nh tinh m·ªõi ƒë∆∞·ª£c g√°n ch√≠nh x√°c v√† n√≥ ƒë∆∞·ª£c t√≠ch h·ª£p ƒë√∫ng v√†o h·ªá th·ªëng.\\n\\nT√†i li·ªáu c≈©ng r·∫•t quan tr·ªçng. Vi·ªác c·∫≠p nh·∫≠t c√°c nh·∫≠n x√©t v√† b·∫•t k·ª≥ t√†i li·ªáu li√™n quan n√†o ƒë·ªÉ ph·∫£n √°nh vi·ªác b·ªï sung h√†nh tinh m·ªõi s·∫Ω gi√∫p c√°c nh√† ph√°t tri·ªÉn trong t∆∞∆°ng lai hi·ªÉu ƒë∆∞·ª£c nh·ªØng thay ƒë·ªïi ƒë√£ th·ª±c hi·ªán. ƒêi·ªÅu n√†y bao g·ªìm vi·ªác gi·∫£i th√≠ch quy ∆∞·ªõc ƒë·∫∑t t√™n v√† c√°c r√†ng bu·ªôc v·ªÅ kho·∫£ng c√°ch.\\n\\nCu·ªëi c√πng, t√¥i n√™n nghƒ© v·ªÅ nh·ªØng t√°c ƒë·ªông c·ªßa giao di·ªán ng∆∞·ªùi d√πng n·∫øu m·∫£ng h√†nh tinh ƒë∆∞·ª£c s·ª≠ d·ª•ng trong m·ªôt th√†nh ph·∫ßn UI. H√†nh tinh m·ªõi ph·∫£i ƒë∆∞·ª£c hi·ªÉn th·ªã ch√≠nh x√°c, v·ªõi t√™n v√† kho·∫£ng c√°ch c·ªßa n√≥ ƒë∆∞·ª£c th·ªÉ hi·ªán m·ªôt c√°ch ch√≠nh x√°c. ƒêi·ªÅu n√†y c√≥ th·ªÉ li√™n quan ƒë·∫øn vi·ªác c·∫≠p nh·∫≠t c√°c m·∫´u ho·∫∑c ch·∫ø ƒë·ªô xem ƒë·ªÉ ph√π h·ª£p v·ªõi d·ªØ li·ªáu b·ªï sung.\\n\\nT√≥m l·∫°i, vi·ªác th√™m m·ªôt h√†nh tinh m·ªõi bao g·ªìm vi·ªác t·∫°o m·ªôt ƒë·ªëi t∆∞·ª£ng m·ªõi v·ªõi m·ªôt c√°i t√™n ph√π h·ª£p v·ªõi th·∫ßn tho·∫°i, ƒë·∫£m b·∫£o kho·∫£ng c√°ch c·ªßa n√≥ n·∫±m trong ph·∫°m vi Sao Th·ªßy-Sao Di√™m V∆∞∆°ng, c·∫≠p nh·∫≠t m·∫£ng, x·ª≠ l√Ω c√°c l·ªói ti·ªÅm ·∫©n, ki·ªÉm tra c√°c thay ƒë·ªïi, c·∫≠p nh·∫≠t t√†i li·ªáu v√† xem x√©t c√°c t√°c ƒë·ªông c·ªßa UI. B·∫±ng c√°ch l√†m theo c√°c b∆∞·ªõc n√†y m·ªôt c√°ch c√≥ ph∆∞∆°ng ph√°p, t√¥i c√≥ th·ªÉ t√≠ch h·ª£p th√†nh c√¥ng h√†nh tinh m·ªõi v√†o h·ªá th·ªëng hi·ªán c√≥.</reasoning>\\n\\n<answer>```javascript\\n// M·∫£ng c√°c h√†nh tinh hi·ªán c√≥\\nlet planets = [\\n    { name: \\'Mercury\\', distanceFromSun: 58 }, // t√≠nh b·∫±ng tri·ªáu km\\n    { name: \\'Venus\\', distanceFromSun: 108 },\\n    { name: \\'Earth\\', distanceFromSun: 149.6 },\\n    { name: \\'Mars\\', distanceFromSun: 227.9 },\\n    { name: \\'Jupiter\\', distanceFromSun: 778.3 },\\n    { name: \\'Saturn\\', distanceFromSun: 1427 },\\n    { name: \\'Uranus\\', distanceFromSun: 2871 },\\n    { name: \\'Neptune\\', distanceFromSun: 4497 },\\n    { name: \\'Pluto\\', distanceFromSun: 5900 }\\n];\\n\\n// H√†m th√™m m·ªôt h√†nh tinh m·ªõi\\nfunction addPlanet(newPlanet) {\\n    // X√°c th·ª±c t√™n ch∆∞a ƒë∆∞·ª£c s·ª≠ d·ª•ng\\n    const nameExists = planets.some(planet => planet.name === newPlanet.name);\\n    if (nameExists) {\\n        console.error(`H√†nh tinh t√™n ${newPlanet.name} ƒë√£ t·ªìn t·∫°i.`);\\n        return;\\n    }\\n\\n    // X√°c th·ª±c kho·∫£ng c√°ch n·∫±m trong ph·∫°m vi c·ªßa Sao Th·ªßy v√† Sao Di√™m V∆∞∆°ng\\n    if (newPlanet.distanceFromSun < planets[0].distanceFromSun || newPlanet.distanceFromSun > planets[planets.length - 1].distanceFromSun) {\\n        console.error(`H√†nh tinh ${newPlanet.name} n·∫±m ngo√†i ph·∫°m vi kho·∫£ng c√°ch cho ph√©p.`);\\n        return;\\n    }\\n\\n    // Th√™m h√†nh tinh m·ªõi v√†o m·∫£ng\\n    planets.push(newPlanet);\\n    console.log(`ƒê√£ th√™m th√†nh c√¥ng h√†nh tinh ${newPlanet.name}.`);\\n}\\n\\n// Chi ti·∫øt h√†nh tinh m·ªõi\\nconst newPlanet = {\\n    name: \\'Lysithea\\', // B·∫Øt ngu·ªìn t·ª´ th·∫ßn tho·∫°i c·ªï ƒëi·ªÉn\\n    distanceFromSun: 300 // t√≠nh b·∫±ng tri·ªáu km\\n};\\n\\n// Th√™m h√†nh tinh m·ªõi\\naddPlanet(newPlanet);\\n\\n// M·∫£ng c√°c h√†nh tinh ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t\\nconsole.log(planets);\\n```</answer><|eot_id|>'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "space = tokenizer(\" \", add_special_tokens=False).input_ids[0]\n",
        "tokenizer.decode(\n",
        "    [space if x == -100 else x for x in trainer.train_dataset[6][\"labels\"]]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3enWUM0jV-jV"
      },
      "source": [
        "We can see the System and Instruction prompts are successfully masked!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "a2a7d5f0-4f83-4a63-ce92-ec2f384af156"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = NVIDIA GeForce RTX 4060. Max memory = 7.739 GB.\n",
            "2.301 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "5d386664-e08f-47ab-a0d7-44adaccb2f28"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 5,372 | Num Epochs = 2\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 3\n",
            "\\        /    Total batch size = 6 | Total steps = 1,790\n",
            " \"-____-\"     Number of trainable parameters = 24,313,856\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='1790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   2/1790 : < :, Epoch 0.00/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/project/fine_tuning_model/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:361\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 361\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
            "File \u001b[0;32m~/Documents/project/fine_tuning_model/.venv/lib/python3.12/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m<string>:329\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "File \u001b[0;32m<string>:73\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n",
            "File \u001b[0;32m~/Documents/project/fine_tuning_model/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:2329\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2329\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/project/fine_tuning_model/.venv/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/project/fine_tuning_model/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/project/fine_tuning_model/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "fcbecf7f-b8a1-45d5-f415-eec2bdf96576"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'trainer_stats' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m used_percentage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(used_memory \u001b[38;5;241m/\u001b[39m max_memory \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m lora_percentage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(used_memory_for_lora \u001b[38;5;241m/\u001b[39m max_memory \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtrainer_stats\u001b[49m\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_runtime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds used for training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(trainer_stats\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_runtime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes used for training.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPeak reserved memory = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mused_memory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GB.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer_stats' is not defined"
          ]
        }
      ],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "\n",
        "### Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "2f600133-5925-4348-f39d-f57a2efbd17c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTi·∫øp t·ª•c chu·ªói fibonacci: 1, 1, 2, 3, 5, 8,<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nT·∫°o h√†m fibonacci l·∫•y t·ª´ bi·∫øn fibonacci c√≥ gi√° tr·ªã ban ƒë·∫ßu 0,1:\\n\\nTi·∫øp t·ª•c chu·ªói fibonacci: 1, 1, 2, 3, 5, 8, 13, 21, 34,...\\n\\nTi·∫øp t·ª•c chu·ªói fibonacci: 1, 1, 2, 3, 5, 8, 13, 21,...\\n\\nTi·∫øp t·ª•c chu·ªói fibonacci: 1, 1, 2, 3, 5, 8,...\\n\\nTi·∫øp t·ª•c chu·ªói fibonacci: 1, 1, 2, 3,...\\n\\nTi']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"llama-3.1\",\n",
        ")\n",
        "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Ti·∫øp t·ª•c chu·ªói fibonacci: 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,  # Must add for generation\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs, max_new_tokens=128, use_cache=True, temperature=1.5, min_p=0.1\n",
        ")\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        "You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "e2pEuRb1r2Vg",
        "outputId": "ffee7558-1224-46ba-aaf0-6852c64f91e8"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": \"Ti·∫øp t·ª•c chu·ªói Fibonacci: 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,  # Must add for generation\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "_ = model.generate(\n",
        "    input_ids=inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=128,\n",
        "    use_cache=True,\n",
        "    temperature=1.5,\n",
        "    min_p=0.1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "\n",
        "### Saving, loading finetuned models\n",
        "\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "2b97c884-8e78-44fb-8726-f95e0634058b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "MKX_XKs_BNZR",
        "outputId": "7b1425af-cd0e-4f6a-dffe-e0f342019aa7"
      },
      "outputs": [],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"lora_model\",  # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length=max_seq_length,\n",
        "        dtype=dtype,\n",
        "        load_in_4bit=load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": \"Ti·∫øp t·ª•c chu·ªói fibonacci: 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,  # Must add for generation\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "_ = model.generate(\n",
        "    input_ids=inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=4000,\n",
        "    use_cache=True,\n",
        "    temperature=0.5,\n",
        "    min_p=0.1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQMjaNrjsU5_"
      },
      "source": [
        "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFfaXG0WsQuE",
        "outputId": "6d08819b-1169-42b6-e008-8e333998def5"
      },
      "outputs": [],
      "source": [
        "if True:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"lora_model\",  # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit=load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43qBkEbXzCaN"
      },
      "source": [
        "# Ho·∫∑c d√πng c√°ch d∆∞·ªõi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOheGXS0zAf5",
        "outputId": "e6af13ef-c112-4391-cfe1-6578d4a6c30e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load model v√† tokenizer\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    \"load_models\",\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")\n",
        "\n",
        "# Merge LoRA v√†o base model\n",
        "merged_model = model.merge_and_unload()  # ‚Üê B∆∞·ªõc quan tr·ªçng!\n",
        "\n",
        "# L∆∞u model ƒë√£ merge\n",
        "merged_model.save_pretrained(\n",
        "    \"output/merged_model\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=False,\n",
        ")\n",
        "tokenizer.save_pretrained(\"output/merged_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if True:\n",
        "    model.save_pretrained(\n",
        "        save_directory=\"ChunThinker/lora_model_vllm\",\n",
        "        tokenizer=tokenizer,  # L∆∞u tokenizer ri√™ng\n",
        "        save_method=\"merged_16bit\",  # Merge adapter v√†o base model\n",
        "        safe_serialization=True,  # B·∫Øt bu·ªôc ƒë·ªÉ tr√°nh l·ªói\n",
        "    )\n",
        "if False:\n",
        "    model.push_to_hub_merged(\n",
        "        \"hf/model\", tokenizer, save_method=\"merged_16bit\", token=\"\"\n",
        "    )\n",
        "\n",
        "# Merge to 4bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\n",
        "        \"model\",\n",
        "        tokenizer,\n",
        "        save_method=\"merged_4bit\",\n",
        "    )\n",
        "if False:\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_4bit\", token=\"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained_merged(\n",
        "        \"model\",\n",
        "        tokenizer,\n",
        "        save_method=\"lora\",\n",
        "    )\n",
        "if False:\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"lora\", token=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "\n",
        "- `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "- `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "- `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\n",
        "        \"model\",\n",
        "        tokenizer,\n",
        "    )\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, token=\"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if True:\n",
        "    model.save_pretrained(\n",
        "        \"output/model/gguf\", tokenizer, quantization_method=\"f16\"\n",
        "    )\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method=\"f16\", token=\"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\")\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", tokenizer, quantization_method=\"q4_k_m\", token=\"\"\n",
        "    )\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\",  # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method=[\n",
        "            \"q4_k_m\",\n",
        "            \"q8_0\",\n",
        "            \"q5_k_m\",\n",
        "        ],\n",
        "        token=\"\",  # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05361a49bec247a989b92d0273f8a8a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_261f2d39bc1247b1b7ec557c92626f44",
            "max": 11372,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a8544ba4489494097d446606349043a",
            "value": 11372
          }
        },
        "070ca8d4788040439d70b289123ed7f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "12184c89275c42a59f3c5f7a512cf8e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14c6d25d2c6a433ca772fbf8c95061d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14fcca8d922a450c8afcab53e5aa65e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14c6d25d2c6a433ca772fbf8c95061d8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8b6a38b01cd44c73859da2e459e8a80f",
            "value": "Map:‚Äá100%"
          }
        },
        "1a92e911567146e5908e7e680ecd4905": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37a5403876f8466ba0cf676ca1a3f345",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b8cb050625fe44f59cc75fb4669ee287",
            "value": "‚Äá11372/11372‚Äá[00:14&lt;00:00,‚Äá720.19‚Äáexamples/s]"
          }
        },
        "248d471ae26842e7947a1c4087f8097e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "261f2d39bc1247b1b7ec557c92626f44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "270314604ce74721b857c5204d41b283": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29f24afedecc43db95ac4e7ab1fa01e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dd1efc8384f4356ade13143f9774bdd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8dc92c23eff34236aedc83724db607b7",
            "value": "‚Äá11372/11372‚Äá[00:23&lt;00:00,‚Äá633.90‚Äáexamples/s]"
          }
        },
        "326c8c02704742aba6eb9906b6ab565d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7720d0627c7e4770bb8417ca3d53e82c",
              "IPY_MODEL_05361a49bec247a989b92d0273f8a8a6",
              "IPY_MODEL_fda2b821efb347258145ac593cf49c0d"
            ],
            "layout": "IPY_MODEL_a830b7f0feda4748978b95024204e1bc"
          }
        },
        "34c9e9cfbf104c90a8d0b8ec30021edd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37a5403876f8466ba0cf676ca1a3f345": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a7731db739e4674a93db274fb135d7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3db45cd840274a8f8b4835d4b594839d",
            "max": 11372,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5bc57b3afb94473a994077567332651",
            "value": 11372
          }
        },
        "3cc9bdfb85704b17a64f7097ebf4a44b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9705c35042184d2db14ebae9d9f1ecfa",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_270314604ce74721b857c5204d41b283",
            "value": "Tokenizing‚Äátrain‚Äádataset‚Äá(num_proc=2):‚Äá100%"
          }
        },
        "3db45cd840274a8f8b4835d4b594839d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dd1efc8384f4356ade13143f9774bdd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40edbd257909455b95f13c75ccec8661": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "467ba48099b0486798322ceb4b206bf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df7d41a027c8446ebb11bd0210c5c103",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c484ba18d3134eb8815498b2328717bb",
            "value": "Map:‚Äá100%"
          }
        },
        "4d14e81bc0c642ad833e61f764f521e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee420abea7814a7c874cc4d30d53778f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_34c9e9cfbf104c90a8d0b8ec30021edd",
            "value": "‚Äá11372/11372‚Äá[00:03&lt;00:00,‚Äá7321.18‚Äáexamples/s]"
          }
        },
        "51fc4cde067746cda57fbc2b64e02d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a8544ba4489494097d446606349043a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e5ed953d2a741cbb9ba6d954a1e7c18": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7720d0627c7e4770bb8417ca3d53e82c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9884af3cc32647acaf3b9de913d4d21d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_51fc4cde067746cda57fbc2b64e02d40",
            "value": "Tokenizing‚Äátrain‚Äádataset‚Äá(num_proc=2):‚Äá100%"
          }
        },
        "796191af9a0d4bab9377d13a05da4b5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7780eaba753460eb5d03a48cf0c371a",
            "max": 11372,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_070ca8d4788040439d70b289123ed7f5",
            "value": 11372
          }
        },
        "7f69c2f0ad70491b82ed1b550c2741b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_467ba48099b0486798322ceb4b206bf5",
              "IPY_MODEL_3a7731db739e4674a93db274fb135d7e",
              "IPY_MODEL_1a92e911567146e5908e7e680ecd4905"
            ],
            "layout": "IPY_MODEL_c9f860dd5ef24a42988a1adb79a1a30b"
          }
        },
        "8b6a38b01cd44c73859da2e459e8a80f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8dc92c23eff34236aedc83724db607b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e4c415dfe09453bbe50816fc815b1a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40edbd257909455b95f13c75ccec8661",
            "max": 11372,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0072ad5352348c3a551f1c6818445d4",
            "value": 11372
          }
        },
        "9705c35042184d2db14ebae9d9f1ecfa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9884af3cc32647acaf3b9de913d4d21d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a211854308d4cf896022aead86a6fc6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0072ad5352348c3a551f1c6818445d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3ec2615a9d343329f17a4a2278a8a14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3cc9bdfb85704b17a64f7097ebf4a44b",
              "IPY_MODEL_e3a9a42cae3d419e95ffedc046a9887c",
              "IPY_MODEL_29f24afedecc43db95ac4e7ab1fa01e0"
            ],
            "layout": "IPY_MODEL_be00b0e96faf46098160b2dd33b448cc"
          }
        },
        "a5bc57b3afb94473a994077567332651": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7780eaba753460eb5d03a48cf0c371a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a830b7f0feda4748978b95024204e1bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8cb050625fe44f59cc75fb4669ee287": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb2e1e90765e4ce1a02cf7e8a9f6d881": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be00b0e96faf46098160b2dd33b448cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2c710d06dd04428a8595296c52828bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12184c89275c42a59f3c5f7a512cf8e6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c65192e89e6046299174394d62c31002",
            "value": "Applying‚Äáchat‚Äátemplate‚Äáto‚Äátrain‚Äádataset‚Äá(num_proc=2):‚Äá100%"
          }
        },
        "c484ba18d3134eb8815498b2328717bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c65192e89e6046299174394d62c31002": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7e834a42885469cb4e291a0c78ede14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14fcca8d922a450c8afcab53e5aa65e1",
              "IPY_MODEL_796191af9a0d4bab9377d13a05da4b5f",
              "IPY_MODEL_e1d893dc93374ff389b9283019d3bbea"
            ],
            "layout": "IPY_MODEL_248d471ae26842e7947a1c4087f8097e"
          }
        },
        "c9f860dd5ef24a42988a1adb79a1a30b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd1085f167374f8f96a4fd61feb125c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d664254d90224b0d9738d7fc92dac1c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbd1c96a6c4a4b3a81457436407e75ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df7d41a027c8446ebb11bd0210c5c103": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1d893dc93374ff389b9283019d3bbea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb2e1e90765e4ce1a02cf7e8a9f6d881",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e84444abd7e843328a762cade219777d",
            "value": "‚Äá11372/11372‚Äá[00:02&lt;00:00,‚Äá6281.70‚Äáexamples/s]"
          }
        },
        "e3a9a42cae3d419e95ffedc046a9887c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d664254d90224b0d9738d7fc92dac1c9",
            "max": 11372,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd1085f167374f8f96a4fd61feb125c9",
            "value": 11372
          }
        },
        "e84444abd7e843328a762cade219777d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee420abea7814a7c874cc4d30d53778f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f66ceb5f96fe4ae38193a4a6f9a4fd60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2c710d06dd04428a8595296c52828bb",
              "IPY_MODEL_8e4c415dfe09453bbe50816fc815b1a2",
              "IPY_MODEL_4d14e81bc0c642ad833e61f764f521e3"
            ],
            "layout": "IPY_MODEL_6e5ed953d2a741cbb9ba6d954a1e7c18"
          }
        },
        "fda2b821efb347258145ac593cf49c0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a211854308d4cf896022aead86a6fc6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_dbd1c96a6c4a4b3a81457436407e75ec",
            "value": "‚Äá11372/11372‚Äá[00:59&lt;00:00,‚Äá188.99‚Äáexamples/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
