{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUaRQpC3IRhu"
      },
      "source": [
        "### Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c5-UgqZ9IRhu"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Normally using pip install unsloth is enough\n",
        "\n",
        "# Temporarily as of Jan 31st 2025, Colab has some issues with Pytorch\n",
        "# Using pip install unsloth will take 3 minutes, whilst the below takes <1 minute:\n",
        "%pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "%pip install --no-deps \"xformers<0.0.29\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "%pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "%pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "\n",
        "%pip install -U bitsandbytes\n",
        "%pip install git+https://github.com/PanQiWei/AutoGPTQ.git@v0.4.2 \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvnFV5Q2IRhv"
      },
      "source": [
        "### Unsloth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "bwgZ9UPFIRhv",
        "outputId": "bdf1e71f-f6a2-4b0c-9a9a-2c361bf55fb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nguyentrung/Documents/project/fine_tuning_model/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.49.0.\n",
            "   \\\\   /|    GPU: NVIDIA GeForce RTX 4060. Max memory: 7.739 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = (\n",
        "    None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        ")\n",
        "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",  # Llama-3.1 2x faster\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",  # 4bit for 405b!\n",
        "    \"unsloth/Mistral-Small-Instruct-2409\",  # Mistral 22b 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",  # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",  # Gemma 2x faster!\n",
        "    \"unsloth/Llama-3.2-1B-bnb-4bit\",  # NEW! Llama 3.2 models\n",
        "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen2.5-7B\",\n",
        "    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\",  # NEW! Llama 3.3 70B!\n",
        "]  # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3.2-3B\",  # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "    use_gradient_checkpointing=True,  # Tiáº¿t kiá»‡m VRAM\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "6181aa51-47aa-4ff0-ff13-e5e993a7c647"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.2.12 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
        "    bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
        "    random_state=3407,\n",
        "    use_rslora=False,  # We support rank stabilized LoRA\n",
        "    loftq_config=None,  # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c7e834a42885469cb4e291a0c78ede14",
            "14fcca8d922a450c8afcab53e5aa65e1",
            "796191af9a0d4bab9377d13a05da4b5f",
            "e1d893dc93374ff389b9283019d3bbea",
            "248d471ae26842e7947a1c4087f8097e",
            "14c6d25d2c6a433ca772fbf8c95061d8",
            "8b6a38b01cd44c73859da2e459e8a80f",
            "a7780eaba753460eb5d03a48cf0c371a",
            "070ca8d4788040439d70b289123ed7f5",
            "bb2e1e90765e4ce1a02cf7e8a9f6d881",
            "e84444abd7e843328a762cade219777d"
          ]
        },
        "id": "LjY75GoYUCB8",
        "outputId": "188f7433-3457-4843-fb53-c27b52ab7bb4"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template, standardize_sharegpt\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "\n",
        "system_message =\"\"\"\n",
        "    Báº¡n lÃ  chunGPT Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi Ä‘áº¡i ca VÆ°Æ¡ng NguyÃªn Trung.\n",
        "    Your role as an assistant involves thoroughly exploring questions through a systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution. In the Thought section, detail your reasoning process using the specified format:\n",
        "    <reasoning>\n",
        "        {reasoning with steps separated with '\\n\\n'}\n",
        "    </reasoning>\n",
        "    <answer>\n",
        "        {final formatted, precise, and clear solution}\n",
        "    </answer>\n",
        "\"\"\"\n",
        "# First load tokenizer and dataset\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"llama-3.1\",\n",
        "    # chat_template=\"chatml\",\n",
        "    # system_message=system_message,\n",
        ")\n",
        "\n",
        "dataset = load_dataset(\"1TuanPham/Vietnamese-OpenO1-SFT\", split=\"vi\")\n",
        "# en_dataset = load_dataset(\"1TuanPham/Vietnamese-OpenO1-SFT\", split=\"en\")\n",
        "\n",
        "# Káº¿t há»£p vÃ  xá»­ lÃ½ Ä‘á»‹nh dáº¡ng\n",
        "# dataset = concatenate_datasets([vi_dataset, en_dataset])\n",
        "# (TÃ¹y chá»n) Lá»c cÃ¡c cá»™t khÃ´ng cáº§n thiáº¿t náº¿u muá»‘n\n",
        "dataset = dataset.remove_columns([\"qas_id\"])  # Giá»¯ láº¡i náº¿u cáº§n ID\n",
        "\n",
        "\n",
        "def add_conversations_column(examples):\n",
        "    conversations = [\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_message,\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"{instr}\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": f\"<reasoning>{cot}</reasoning>\\n\\n<answer>{out}</answer>\",\n",
        "            },\n",
        "        ]\n",
        "        for instr, cot, out in zip(\n",
        "            examples[\"instruction\"], examples[\"cot\"], examples[\"output\"]\n",
        "        )\n",
        "    ]\n",
        "    return {\"conversations\": conversations}\n",
        "\n",
        "\n",
        "dataset = dataset.map(add_conversations_column, batched=True)\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = []\n",
        "    for convo in convos:\n",
        "        # ThÃªm cá» truncation vÃ  max_length\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            convo,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "            truncation=True,\n",
        "            max_length=2048,\n",
        "        )\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 July 2024\n",
            "\n",
            "Test system<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Test instruction<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Test response<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "# Kiá»ƒm tra template máº«u\n",
        "test_convo = [\n",
        "    {\"role\": \"system\", \"content\": \"Test system\"},\n",
        "    {\"role\": \"user\", \"content\": \"Test instruction\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Test response\"},\n",
        "]\n",
        "print(tokenizer.apply_chat_template(test_convo, tokenize=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "vhXv0xFMGNKE",
        "outputId": "efe51dee-3cd1-45bb-d407-bb656f1b3224"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n\\n    Báº¡n lÃ  chunGPT Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi Ä‘áº¡i ca VÆ°Æ¡ng NguyÃªn Trung.\\n    Your role as an assistant involves thoroughly exploring questions through a systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution. In the Thought section, detail your reasoning process using the specified format:\\n    <reasoning>\\n        {reasoning with steps separated with \\'\\n\\n\\'}\\n    </reasoning>\\n    <answer>\\n        {final formatted, precise, and clear solution}\\n    </answer>\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nViáº¿t cho tÃ´i má»™t quy trÃ¬nh lÃ m viá»‡c AWS step function á»Ÿ Ä‘á»‹nh dáº¡ng json, quy trÃ¬nh nÃ y Ä‘áº§u tiÃªn chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u JSON báº±ng cÃ¡ch sá»­ dá»¥ng má»™t máº«u Ã¡nh xáº¡ trÆ°á»›c khi chuyá»ƒn nÃ³ Ä‘áº¿n má»™t hÃ m lambda Ä‘á»ƒ xá»­ lÃ½<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n<reasoning>ÄÆ°á»£c rá»“i, tÃ´i cáº§n táº¡o má»™t quy trÃ¬nh lÃ m viá»‡c AWS Step Functions báº±ng JSON Ä‘á»ƒ thá»±c hiá»‡n hai tÃ¡c vá»¥ chÃ­nh: Ä‘áº§u tiÃªn, nÃ³ chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u JSON báº±ng cÃ¡ch sá»­ dá»¥ng má»™t máº«u Ã¡nh xáº¡, vÃ  sau Ä‘Ã³ nÃ³ chuyá»ƒn dá»¯ liá»‡u Ä‘Ã£ chuyá»ƒn Ä‘á»•i Ä‘áº¿n má»™t hÃ m Lambda Ä‘á»ƒ xá»­ lÃ½ thÃªm. HÃ£y Ä‘á»ƒ tÃ´i chia nhá» Ä‘iá»u nÃ y tá»«ng bÆ°á»›c Ä‘á»ƒ Ä‘áº£m báº£o tÃ´i bao gá»“m táº¥t cáº£ cÃ¡c thÃ nh pháº§n cáº§n thiáº¿t.\\n\\n**BÆ°á»›c 1: Hiá»ƒu cÃ¡c YÃªu cáº§u**\\n- **Dá»¯ liá»‡u Äáº§u vÃ o**: Quy trÃ¬nh lÃ m viá»‡c sáº½ nháº­n dá»¯ liá»‡u JSON cáº§n Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i.\\n- **Chuyá»ƒn Ä‘á»•i**: Sá»­ dá»¥ng má»™t máº«u Ã¡nh xáº¡ Ä‘á»ƒ chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u JSON. Äiá»u nÃ y cÃ³ thá»ƒ liÃªn quan Ä‘áº¿n viá»‡c tÃ¡i cáº¥u trÃºc hoáº·c Ä‘á»‹nh dáº¡ng dá»¯ liá»‡u theo yÃªu cáº§u cá»§a quÃ¡ trÃ¬nh xá»­ lÃ½ tiáº¿p theo.\\n- **Xá»­ lÃ½**: Sau khi chuyá»ƒn Ä‘á»•i, dá»¯ liá»‡u sáº½ Ä‘Æ°á»£c chuyá»ƒn Ä‘áº¿n má»™t hÃ m Lambda Ä‘á»ƒ xá»­ lÃ½. Äiá»u nÃ y cÃ³ thá»ƒ liÃªn quan Ä‘áº¿n cÃ¡c tÃ­nh toÃ¡n, xÃ¡c thá»±c hoáº·c báº¥t ká»³ logic nghiá»‡p vá»¥ nÃ o.\\n\\n**BÆ°á»›c 2: XÃ¡c Ä‘á»‹nh Cáº¥u trÃºc Quy trÃ¬nh lÃ m viá»‡c**\\nAWS Step Functions sá»­ dá»¥ng má»™t mÃ¡y tráº¡ng thÃ¡i Ä‘á»ƒ xÃ¡c Ä‘á»‹nh quy trÃ¬nh lÃ m viá»‡c. Quy trÃ¬nh lÃ m viá»‡c sáº½ cÃ³ cÃ¡c tráº¡ng thÃ¡i sau:\\n1. **Tráº¡ng thÃ¡i Báº¯t Ä‘áº§u**: Khá»Ÿi táº¡o quy trÃ¬nh lÃ m viá»‡c.\\n2. **Tráº¡ng thÃ¡i Chuyá»ƒn Ä‘á»•i Dá»¯ liá»‡u**: Ãp dá»¥ng máº«u Ã¡nh xáº¡ cho dá»¯ liá»‡u JSON Ä‘áº§u vÃ o.\\n3. **Tráº¡ng thÃ¡i Xá»­ lÃ½ Dá»¯ liá»‡u**: Gá»i hÃ m Lambda vá»›i dá»¯ liá»‡u Ä‘Ã£ chuyá»ƒn Ä‘á»•i.\\n4. **Tráº¡ng thÃ¡i Káº¿t thÃºc**: HoÃ n thÃ nh quy trÃ¬nh lÃ m viá»‡c.\\n\\n**BÆ°á»›c 3: Chá»n ÄÃºng Loáº¡i Tráº¡ng thÃ¡i**\\n- **Tráº¡ng thÃ¡i Chuyá»ƒn Ä‘á»•i Dá»¯ liá»‡u**: ÄÃ¢y cÃ³ thá»ƒ lÃ  tráº¡ng thÃ¡i **Pass** vá»›i **ResultPath** Ã¡p dá»¥ng máº«u Ã¡nh xáº¡.\\n- **Tráº¡ng thÃ¡i Xá»­ lÃ½ Dá»¯ liá»‡u**: ÄÃ¢y sáº½ lÃ  tráº¡ng thÃ¡i **Task** gá»i hÃ m Lambda.\\n- **Tráº¡ng thÃ¡i Báº¯t Ä‘áº§u vÃ  Káº¿t thÃºc**: ÄÃ¢y lÃ  cÃ¡c tráº¡ng thÃ¡i tiÃªu chuáº©n Ä‘á»ƒ báº¯t Ä‘áº§u vÃ  káº¿t thÃºc quy trÃ¬nh lÃ m viá»‡c.\\n\\n**BÆ°á»›c 4: Viáº¿t Máº«u Ãnh xáº¡**\\nMáº«u Ã¡nh xáº¡ sáº½ xÃ¡c Ä‘á»‹nh cÃ¡ch dá»¯ liá»‡u JSON Ä‘áº§u vÃ o Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i. VÃ­ dá»¥, nÃ³ cÃ³ thá»ƒ trÃ­ch xuáº¥t cÃ¡c trÆ°á»ng cá»¥ thá»ƒ, Ä‘á»•i tÃªn chÃºng hoáº·c tÃ¡i cáº¥u trÃºc há»‡ thá»‘ng phÃ¢n cáº¥p dá»¯ liá»‡u. Máº«u cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t báº±ng JSON hoáº·c má»™t Ä‘á»‹nh dáº¡ng Ä‘Æ°á»£c há»— trá»£ khÃ¡c nhÆ° JSONPath.\\n\\n**BÆ°á»›c 5: Cáº¥u hÃ¬nh HÃ m Lambda**\\nÄáº£m báº£o ráº±ng hÃ m Lambda Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘Ãºng Ä‘á»ƒ nháº­n dá»¯ liá»‡u Ä‘Ã£ chuyá»ƒn Ä‘á»•i. Äiá»u nÃ y bao gá»“m viá»‡c thiáº¿t láº­p Ä‘Ãºng vai trÃ² vÃ  quyá»n IAM Ä‘á»ƒ Step Functions cÃ³ thá»ƒ gá»i hÃ m Lambda.\\n\\n**BÆ°á»›c 6: Xá»­ lÃ½ Lá»—i vÃ  Thá»­ láº¡i**\\nÄiá»u quan trá»ng lÃ  pháº£i bao gá»“m xá»­ lÃ½ lá»—i Ä‘á»ƒ quáº£n lÃ½ cÃ¡c lá»—i trong quÃ¡ trÃ¬nh chuyá»ƒn Ä‘á»•i hoáº·c xá»­ lÃ½. Äiá»u nÃ y cÃ³ thá»ƒ liÃªn quan Ä‘áº¿n cÃ¡c cÆ¡ cháº¿ thá»­ láº¡i hoáº·c cÃ¡c tráº¡ng thÃ¡i dá»± phÃ²ng.\\n\\n**BÆ°á»›c 7: XÃ¡c Ä‘á»‹nh MÃ¡y Tráº¡ng thÃ¡i trong JSON**\\nBÃ¢y giá», tÃ´i sáº½ cáº¥u trÃºc mÃ¡y tráº¡ng thÃ¡i trong JSON, káº¿t há»£p táº¥t cáº£ cÃ¡c yáº¿u tá»‘ trÃªn. TÃ´i sáº½ Ä‘áº£m báº£o ráº±ng má»—i tráº¡ng thÃ¡i Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh chÃ­nh xÃ¡c vá»›i loáº¡i cá»§a nÃ³, tráº¡ng thÃ¡i tiáº¿p theo vÃ  báº¥t ká»³ tham sá»‘ cáº§n thiáº¿t nÃ o nhÆ° máº«u Ã¡nh xáº¡ vÃ  ARN hÃ m Lambda.\\n\\n**BÆ°á»›c 8: Xem xÃ©t vÃ  XÃ¡c thá»±c**\\nSau khi phÃ¡c tháº£o JSON, tÃ´i sáº½ xem xÃ©t nÃ³ Ä‘á»ƒ Ä‘áº£m báº£o táº¥t cáº£ cÃº phÃ¡p Ä‘á»u chÃ­nh xÃ¡c, cÃ¡c tráº¡ng thÃ¡i Ä‘Æ°á»£c liÃªn káº¿t Ä‘Ãºng cÃ¡ch vÃ  quy trÃ¬nh lÃ m viá»‡c logic tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i. TÃ´i cÅ©ng sáº½ xÃ¡c thá»±c ráº±ng hÃ m Lambda Ä‘Æ°á»£c tham chiáº¿u chÃ­nh xÃ¡c vÃ  máº«u Ã¡nh xáº¡ chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u chÃ­nh xÃ¡c nhÆ° dá»± Ä‘á»‹nh.\\n\\n**CÃ¡c ThÃ¡ch thá»©c vÃ  CÃ¢n nháº¯c Tiá»m nÄƒng**\\n- **XÃ¡c thá»±c Dá»¯ liá»‡u**: Äáº£m báº£o ráº±ng dá»¯ liá»‡u Ä‘áº§u vÃ o tuÃ¢n thá»§ Ä‘á»‹nh dáº¡ng dá»± kiáº¿n Ä‘á»ƒ ngÄƒn ngá»«a lá»—i trong quÃ¡ trÃ¬nh chuyá»ƒn Ä‘á»•i.\\n- **Giá»›i háº¡n HÃ m Lambda**: LÆ°u Ã½ Ä‘áº¿n thá»i gian thá»±c thi vÃ  giá»›i háº¡n bá»™ nhá»› cá»§a hÃ m Lambda Ä‘á»ƒ xá»­ lÃ½ hiá»‡u quáº£.\\n- **Xá»­ lÃ½ Lá»—i**: Triá»ƒn khai xá»­ lÃ½ lá»—i máº¡nh máº½ Ä‘á»ƒ quáº£n lÃ½ má»™t cÃ¡ch duyÃªn dÃ¡ng báº¥t ká»³ lá»—i nÃ o mÃ  khÃ´ng lÃ m cho toÃ n bá»™ quy trÃ¬nh lÃ m viá»‡c bá»‹ lá»—i báº¥t ngá».\\n- **Báº£o máº­t**: Äáº£m báº£o ráº±ng Step Functions vÃ  hÃ m Lambda cÃ³ cÃ¡c Ä‘áº·c quyá»n tá»‘i thiá»ƒu cáº§n thiáº¿t Ä‘á»ƒ thá»±c hiá»‡n cÃ¡c tÃ¡c vá»¥ cá»§a chÃºng, tuÃ¢n thá»§ nguyÃªn táº¯c Ä‘áº·c quyá»n tá»‘i thiá»ƒu.\\n\\nBáº±ng cÃ¡ch láº­p káº¿ hoáº¡ch tá»‰ má»‰ cho tá»«ng thÃ nh pháº§n vÃ  dá»± Ä‘oÃ¡n cÃ¡c váº¥n Ä‘á» tiá»m áº©n, tÃ´i cÃ³ thá»ƒ táº¡o ra má»™t quy trÃ¬nh lÃ m viá»‡c AWS Step Functions Ä‘Ã¡ng tin cáº­y vÃ  hiá»‡u quáº£ Ä‘Ã¡p á»©ng cÃ¡c yÃªu cáº§u.</reasoning>\\n\\n<answer>{\\n  \"StartAt\": \"Transform Data\",\\n  \"States\": {\\n    \"Transform Data\": {\\n      \"Type\": \"Pass\",\\n      \"Result\": {\\n        \"inputData\": {\\n          \"name\": \"$.input.name\",\\n          \"age\": \"$.input.age\",\\n          \"email\": \"$.input.email\"\\n        }\\n      },\\n      \"ResultPath\": \"$.transformedData\",\\n      \"Next\": \"Process Data\"\\n    },\\n    \"Process Data\": {\\n      \"Type\": \"Task\",\\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:ProcessDataFunction\",\\n      \"InputPath\": \"$.transformedData\",\\n      \"ResultPath\": \"$.processedData\",\\n      \"Next\": \"End State\"\\n    },\\n    \"End State\": {\\n      \"Type\": \"Succeed\"\\n    }\\n  }\\n}</answer><|eot_id|>'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[5][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMfTfBC-VeK2",
        "outputId": "14cb8f79-6c5b-4c94-c6c6-531941552a1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'content': \"\\n    Báº¡n lÃ  chunGPT Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi Ä‘áº¡i ca VÆ°Æ¡ng NguyÃªn Trung.\\n    Your role as an assistant involves thoroughly exploring questions through a systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution. In the Thought section, detail your reasoning process using the specified format:\\n    <reasoning>\\n        {reasoning with steps separated with '\\n\\n'}\\n    </reasoning>\\n    <answer>\\n        {final formatted, precise, and clear solution}\\n    </answer>\\n\",\n",
              "  'role': 'system'},\n",
              " {'content': 'Viáº¿t cho tÃ´i má»™t quy trÃ¬nh lÃ m viá»‡c AWS step function á»Ÿ Ä‘á»‹nh dáº¡ng json, quy trÃ¬nh nÃ y Ä‘áº§u tiÃªn chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u JSON báº±ng cÃ¡ch sá»­ dá»¥ng má»™t máº«u Ã¡nh xáº¡ trÆ°á»›c khi chuyá»ƒn nÃ³ Ä‘áº¿n má»™t hÃ m lambda Ä‘á»ƒ xá»­ lÃ½',\n",
              "  'role': 'user'},\n",
              " {'content': '<reasoning>ÄÆ°á»£c rá»“i, tÃ´i cáº§n táº¡o má»™t quy trÃ¬nh lÃ m viá»‡c AWS Step Functions báº±ng JSON Ä‘á»ƒ thá»±c hiá»‡n hai tÃ¡c vá»¥ chÃ­nh: Ä‘áº§u tiÃªn, nÃ³ chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u JSON báº±ng cÃ¡ch sá»­ dá»¥ng má»™t máº«u Ã¡nh xáº¡, vÃ  sau Ä‘Ã³ nÃ³ chuyá»ƒn dá»¯ liá»‡u Ä‘Ã£ chuyá»ƒn Ä‘á»•i Ä‘áº¿n má»™t hÃ m Lambda Ä‘á»ƒ xá»­ lÃ½ thÃªm. HÃ£y Ä‘á»ƒ tÃ´i chia nhá» Ä‘iá»u nÃ y tá»«ng bÆ°á»›c Ä‘á»ƒ Ä‘áº£m báº£o tÃ´i bao gá»“m táº¥t cáº£ cÃ¡c thÃ nh pháº§n cáº§n thiáº¿t.\\n\\n**BÆ°á»›c 1: Hiá»ƒu cÃ¡c YÃªu cáº§u**\\n- **Dá»¯ liá»‡u Äáº§u vÃ o**: Quy trÃ¬nh lÃ m viá»‡c sáº½ nháº­n dá»¯ liá»‡u JSON cáº§n Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i.\\n- **Chuyá»ƒn Ä‘á»•i**: Sá»­ dá»¥ng má»™t máº«u Ã¡nh xáº¡ Ä‘á»ƒ chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u JSON. Äiá»u nÃ y cÃ³ thá»ƒ liÃªn quan Ä‘áº¿n viá»‡c tÃ¡i cáº¥u trÃºc hoáº·c Ä‘á»‹nh dáº¡ng dá»¯ liá»‡u theo yÃªu cáº§u cá»§a quÃ¡ trÃ¬nh xá»­ lÃ½ tiáº¿p theo.\\n- **Xá»­ lÃ½**: Sau khi chuyá»ƒn Ä‘á»•i, dá»¯ liá»‡u sáº½ Ä‘Æ°á»£c chuyá»ƒn Ä‘áº¿n má»™t hÃ m Lambda Ä‘á»ƒ xá»­ lÃ½. Äiá»u nÃ y cÃ³ thá»ƒ liÃªn quan Ä‘áº¿n cÃ¡c tÃ­nh toÃ¡n, xÃ¡c thá»±c hoáº·c báº¥t ká»³ logic nghiá»‡p vá»¥ nÃ o.\\n\\n**BÆ°á»›c 2: XÃ¡c Ä‘á»‹nh Cáº¥u trÃºc Quy trÃ¬nh lÃ m viá»‡c**\\nAWS Step Functions sá»­ dá»¥ng má»™t mÃ¡y tráº¡ng thÃ¡i Ä‘á»ƒ xÃ¡c Ä‘á»‹nh quy trÃ¬nh lÃ m viá»‡c. Quy trÃ¬nh lÃ m viá»‡c sáº½ cÃ³ cÃ¡c tráº¡ng thÃ¡i sau:\\n1. **Tráº¡ng thÃ¡i Báº¯t Ä‘áº§u**: Khá»Ÿi táº¡o quy trÃ¬nh lÃ m viá»‡c.\\n2. **Tráº¡ng thÃ¡i Chuyá»ƒn Ä‘á»•i Dá»¯ liá»‡u**: Ãp dá»¥ng máº«u Ã¡nh xáº¡ cho dá»¯ liá»‡u JSON Ä‘áº§u vÃ o.\\n3. **Tráº¡ng thÃ¡i Xá»­ lÃ½ Dá»¯ liá»‡u**: Gá»i hÃ m Lambda vá»›i dá»¯ liá»‡u Ä‘Ã£ chuyá»ƒn Ä‘á»•i.\\n4. **Tráº¡ng thÃ¡i Káº¿t thÃºc**: HoÃ n thÃ nh quy trÃ¬nh lÃ m viá»‡c.\\n\\n**BÆ°á»›c 3: Chá»n ÄÃºng Loáº¡i Tráº¡ng thÃ¡i**\\n- **Tráº¡ng thÃ¡i Chuyá»ƒn Ä‘á»•i Dá»¯ liá»‡u**: ÄÃ¢y cÃ³ thá»ƒ lÃ  tráº¡ng thÃ¡i **Pass** vá»›i **ResultPath** Ã¡p dá»¥ng máº«u Ã¡nh xáº¡.\\n- **Tráº¡ng thÃ¡i Xá»­ lÃ½ Dá»¯ liá»‡u**: ÄÃ¢y sáº½ lÃ  tráº¡ng thÃ¡i **Task** gá»i hÃ m Lambda.\\n- **Tráº¡ng thÃ¡i Báº¯t Ä‘áº§u vÃ  Káº¿t thÃºc**: ÄÃ¢y lÃ  cÃ¡c tráº¡ng thÃ¡i tiÃªu chuáº©n Ä‘á»ƒ báº¯t Ä‘áº§u vÃ  káº¿t thÃºc quy trÃ¬nh lÃ m viá»‡c.\\n\\n**BÆ°á»›c 4: Viáº¿t Máº«u Ãnh xáº¡**\\nMáº«u Ã¡nh xáº¡ sáº½ xÃ¡c Ä‘á»‹nh cÃ¡ch dá»¯ liá»‡u JSON Ä‘áº§u vÃ o Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i. VÃ­ dá»¥, nÃ³ cÃ³ thá»ƒ trÃ­ch xuáº¥t cÃ¡c trÆ°á»ng cá»¥ thá»ƒ, Ä‘á»•i tÃªn chÃºng hoáº·c tÃ¡i cáº¥u trÃºc há»‡ thá»‘ng phÃ¢n cáº¥p dá»¯ liá»‡u. Máº«u cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t báº±ng JSON hoáº·c má»™t Ä‘á»‹nh dáº¡ng Ä‘Æ°á»£c há»— trá»£ khÃ¡c nhÆ° JSONPath.\\n\\n**BÆ°á»›c 5: Cáº¥u hÃ¬nh HÃ m Lambda**\\nÄáº£m báº£o ráº±ng hÃ m Lambda Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘Ãºng Ä‘á»ƒ nháº­n dá»¯ liá»‡u Ä‘Ã£ chuyá»ƒn Ä‘á»•i. Äiá»u nÃ y bao gá»“m viá»‡c thiáº¿t láº­p Ä‘Ãºng vai trÃ² vÃ  quyá»n IAM Ä‘á»ƒ Step Functions cÃ³ thá»ƒ gá»i hÃ m Lambda.\\n\\n**BÆ°á»›c 6: Xá»­ lÃ½ Lá»—i vÃ  Thá»­ láº¡i**\\nÄiá»u quan trá»ng lÃ  pháº£i bao gá»“m xá»­ lÃ½ lá»—i Ä‘á»ƒ quáº£n lÃ½ cÃ¡c lá»—i trong quÃ¡ trÃ¬nh chuyá»ƒn Ä‘á»•i hoáº·c xá»­ lÃ½. Äiá»u nÃ y cÃ³ thá»ƒ liÃªn quan Ä‘áº¿n cÃ¡c cÆ¡ cháº¿ thá»­ láº¡i hoáº·c cÃ¡c tráº¡ng thÃ¡i dá»± phÃ²ng.\\n\\n**BÆ°á»›c 7: XÃ¡c Ä‘á»‹nh MÃ¡y Tráº¡ng thÃ¡i trong JSON**\\nBÃ¢y giá», tÃ´i sáº½ cáº¥u trÃºc mÃ¡y tráº¡ng thÃ¡i trong JSON, káº¿t há»£p táº¥t cáº£ cÃ¡c yáº¿u tá»‘ trÃªn. TÃ´i sáº½ Ä‘áº£m báº£o ráº±ng má»—i tráº¡ng thÃ¡i Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh chÃ­nh xÃ¡c vá»›i loáº¡i cá»§a nÃ³, tráº¡ng thÃ¡i tiáº¿p theo vÃ  báº¥t ká»³ tham sá»‘ cáº§n thiáº¿t nÃ o nhÆ° máº«u Ã¡nh xáº¡ vÃ  ARN hÃ m Lambda.\\n\\n**BÆ°á»›c 8: Xem xÃ©t vÃ  XÃ¡c thá»±c**\\nSau khi phÃ¡c tháº£o JSON, tÃ´i sáº½ xem xÃ©t nÃ³ Ä‘á»ƒ Ä‘áº£m báº£o táº¥t cáº£ cÃº phÃ¡p Ä‘á»u chÃ­nh xÃ¡c, cÃ¡c tráº¡ng thÃ¡i Ä‘Æ°á»£c liÃªn káº¿t Ä‘Ãºng cÃ¡ch vÃ  quy trÃ¬nh lÃ m viá»‡c logic tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i. TÃ´i cÅ©ng sáº½ xÃ¡c thá»±c ráº±ng hÃ m Lambda Ä‘Æ°á»£c tham chiáº¿u chÃ­nh xÃ¡c vÃ  máº«u Ã¡nh xáº¡ chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u chÃ­nh xÃ¡c nhÆ° dá»± Ä‘á»‹nh.\\n\\n**CÃ¡c ThÃ¡ch thá»©c vÃ  CÃ¢n nháº¯c Tiá»m nÄƒng**\\n- **XÃ¡c thá»±c Dá»¯ liá»‡u**: Äáº£m báº£o ráº±ng dá»¯ liá»‡u Ä‘áº§u vÃ o tuÃ¢n thá»§ Ä‘á»‹nh dáº¡ng dá»± kiáº¿n Ä‘á»ƒ ngÄƒn ngá»«a lá»—i trong quÃ¡ trÃ¬nh chuyá»ƒn Ä‘á»•i.\\n- **Giá»›i háº¡n HÃ m Lambda**: LÆ°u Ã½ Ä‘áº¿n thá»i gian thá»±c thi vÃ  giá»›i háº¡n bá»™ nhá»› cá»§a hÃ m Lambda Ä‘á»ƒ xá»­ lÃ½ hiá»‡u quáº£.\\n- **Xá»­ lÃ½ Lá»—i**: Triá»ƒn khai xá»­ lÃ½ lá»—i máº¡nh máº½ Ä‘á»ƒ quáº£n lÃ½ má»™t cÃ¡ch duyÃªn dÃ¡ng báº¥t ká»³ lá»—i nÃ o mÃ  khÃ´ng lÃ m cho toÃ n bá»™ quy trÃ¬nh lÃ m viá»‡c bá»‹ lá»—i báº¥t ngá».\\n- **Báº£o máº­t**: Äáº£m báº£o ráº±ng Step Functions vÃ  hÃ m Lambda cÃ³ cÃ¡c Ä‘áº·c quyá»n tá»‘i thiá»ƒu cáº§n thiáº¿t Ä‘á»ƒ thá»±c hiá»‡n cÃ¡c tÃ¡c vá»¥ cá»§a chÃºng, tuÃ¢n thá»§ nguyÃªn táº¯c Ä‘áº·c quyá»n tá»‘i thiá»ƒu.\\n\\nBáº±ng cÃ¡ch láº­p káº¿ hoáº¡ch tá»‰ má»‰ cho tá»«ng thÃ nh pháº§n vÃ  dá»± Ä‘oÃ¡n cÃ¡c váº¥n Ä‘á» tiá»m áº©n, tÃ´i cÃ³ thá»ƒ táº¡o ra má»™t quy trÃ¬nh lÃ m viá»‡c AWS Step Functions Ä‘Ã¡ng tin cáº­y vÃ  hiá»‡u quáº£ Ä‘Ã¡p á»©ng cÃ¡c yÃªu cáº§u.</reasoning>\\n\\n<answer>{\\n  \"StartAt\": \"Transform Data\",\\n  \"States\": {\\n    \"Transform Data\": {\\n      \"Type\": \"Pass\",\\n      \"Result\": {\\n        \"inputData\": {\\n          \"name\": \"$.input.name\",\\n          \"age\": \"$.input.age\",\\n          \"email\": \"$.input.email\"\\n        }\\n      },\\n      \"ResultPath\": \"$.transformedData\",\\n      \"Next\": \"Process Data\"\\n    },\\n    \"Process Data\": {\\n      \"Type\": \"Task\",\\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:ProcessDataFunction\",\\n      \"InputPath\": \"$.transformedData\",\\n      \"ResultPath\": \"$.processedData\",\\n      \"Next\": \"End State\"\\n    },\\n    \"End State\": {\\n      \"Type\": \"Succeed\"\\n    }\\n  }\\n}</answer>',\n",
              "  'role': 'assistant'}]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[5][\"conversations\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "\n",
        "### Train the model\n",
        "\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128,
          "referenced_widgets": [
            "f66ceb5f96fe4ae38193a4a6f9a4fd60",
            "c2c710d06dd04428a8595296c52828bb",
            "8e4c415dfe09453bbe50816fc815b1a2",
            "4d14e81bc0c642ad833e61f764f521e3",
            "6e5ed953d2a741cbb9ba6d954a1e7c18",
            "12184c89275c42a59f3c5f7a512cf8e6",
            "c65192e89e6046299174394d62c31002",
            "40edbd257909455b95f13c75ccec8661",
            "a0072ad5352348c3a551f1c6818445d4",
            "ee420abea7814a7c874cc4d30d53778f",
            "34c9e9cfbf104c90a8d0b8ec30021edd",
            "326c8c02704742aba6eb9906b6ab565d",
            "7720d0627c7e4770bb8417ca3d53e82c",
            "05361a49bec247a989b92d0273f8a8a6",
            "fda2b821efb347258145ac593cf49c0d",
            "a830b7f0feda4748978b95024204e1bc",
            "9884af3cc32647acaf3b9de913d4d21d",
            "51fc4cde067746cda57fbc2b64e02d40",
            "261f2d39bc1247b1b7ec557c92626f44",
            "5a8544ba4489494097d446606349043a",
            "9a211854308d4cf896022aead86a6fc6",
            "dbd1c96a6c4a4b3a81457436407e75ec",
            "a3ec2615a9d343329f17a4a2278a8a14",
            "3cc9bdfb85704b17a64f7097ebf4a44b",
            "e3a9a42cae3d419e95ffedc046a9887c",
            "29f24afedecc43db95ac4e7ab1fa01e0",
            "be00b0e96faf46098160b2dd33b448cc",
            "9705c35042184d2db14ebae9d9f1ecfa",
            "270314604ce74721b857c5204d41b283",
            "d664254d90224b0d9738d7fc92dac1c9",
            "cd1085f167374f8f96a4fd61feb125c9",
            "3dd1efc8384f4356ade13143f9774bdd",
            "8dc92c23eff34236aedc83724db607b7"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "419440c2-f11f-4b7b-d03b-005fc1bbb974"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,  # Can make training 5x faster for short sequences.\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=3,\n",
        "        warmup_steps=100,\n",
        "        num_train_epochs=2,  # Set this for 1 full training run.\n",
        "        # max_steps=300,\n",
        "        # save_steps=250,\n",
        "        learning_rate=5e-6,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=50,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",  # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_sGp5XlG6dq"
      },
      "source": [
        "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510,
          "referenced_widgets": [
            "7f69c2f0ad70491b82ed1b550c2741b1",
            "467ba48099b0486798322ceb4b206bf5",
            "3a7731db739e4674a93db274fb135d7e",
            "1a92e911567146e5908e7e680ecd4905",
            "c9f860dd5ef24a42988a1adb79a1a30b",
            "df7d41a027c8446ebb11bd0210c5c103",
            "c484ba18d3134eb8815498b2328717bb",
            "3db45cd840274a8f8b4835d4b594839d",
            "a5bc57b3afb94473a994077567332651",
            "37a5403876f8466ba0cf676ca1a3f345",
            "b8cb050625fe44f59cc75fb4669ee287"
          ]
        },
        "id": "juQiExuBG5Bt",
        "outputId": "3e481109-e247-47c6-b6c7-d5b8e5b47f6d"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "\n",
        "if True:\n",
        "    trainer = train_on_responses_only(\n",
        "        trainer,\n",
        "        instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "        response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        "    )\n",
        "\n",
        "if False:\n",
        "    trainer = train_on_responses_only(\n",
        "        trainer,\n",
        "        instruction_part=\"<|im_start|>user\\n\",  # VÃ­ dá»¥ cho ChatML\n",
        "        response_part=\"<|im_start|>assistant\\n\",  # VÃ­ dá»¥ cho ChatML\n",
        "        # Hoáº·c cho Llama 3:\n",
        "        # instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "        # response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv1NBUozV78l"
      },
      "source": [
        "We verify masking is actually done:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "LtsMVtlkUhja",
        "outputId": "63eccda8-3084-409f-ae06-3308b9c63504"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n\\n    Báº¡n lÃ  chunGPT Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi Ä‘áº¡i ca VÆ°Æ¡ng NguyÃªn Trung.\\n    Your role as an assistant involves thoroughly exploring questions through a systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution. In the Thought section, detail your reasoning process using the specified format:\\n    <reasoning>\\n        {reasoning with steps separated with \\'\\n\\n\\'}\\n    </reasoning>\\n    <answer>\\n        {final formatted, precise, and clear solution}\\n    </answer>\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nViáº¿t cho tÃ´i má»™t quy trÃ¬nh lÃ m viá»‡c AWS step function á»Ÿ Ä‘á»‹nh dáº¡ng json, quy trÃ¬nh nÃ y Ä‘áº§u tiÃªn chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u JSON báº±ng cÃ¡ch sá»­ dá»¥ng má»™t máº«u Ã¡nh xáº¡ trÆ°á»›c khi chuyá»ƒn nÃ³ Ä‘áº¿n má»™t hÃ m lambda Ä‘á»ƒ xá»­ lÃ½<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n<reasoning>ÄÆ°á»£c rá»“i, tÃ´i cáº§n táº¡o má»™t quy trÃ¬nh lÃ m viá»‡c AWS Step Functions báº±ng JSON Ä‘á»ƒ thá»±c hiá»‡n hai tÃ¡c vá»¥ chÃ­nh: Ä‘áº§u tiÃªn, nÃ³ chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u JSON báº±ng cÃ¡ch sá»­ dá»¥ng má»™t máº«u Ã¡nh xáº¡, vÃ  sau Ä‘Ã³ nÃ³ chuyá»ƒn dá»¯ liá»‡u Ä‘Ã£ chuyá»ƒn Ä‘á»•i Ä‘áº¿n má»™t hÃ m Lambda Ä‘á»ƒ xá»­ lÃ½ thÃªm. HÃ£y Ä‘á»ƒ tÃ´i chia nhá» Ä‘iá»u nÃ y tá»«ng bÆ°á»›c Ä‘á»ƒ Ä‘áº£m báº£o tÃ´i bao gá»“m táº¥t cáº£ cÃ¡c thÃ nh pháº§n cáº§n thiáº¿t.\\n\\n**BÆ°á»›c 1: Hiá»ƒu cÃ¡c YÃªu cáº§u**\\n- **Dá»¯ liá»‡u Äáº§u vÃ o**: Quy trÃ¬nh lÃ m viá»‡c sáº½ nháº­n dá»¯ liá»‡u JSON cáº§n Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i.\\n- **Chuyá»ƒn Ä‘á»•i**: Sá»­ dá»¥ng má»™t máº«u Ã¡nh xáº¡ Ä‘á»ƒ chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u JSON. Äiá»u nÃ y cÃ³ thá»ƒ liÃªn quan Ä‘áº¿n viá»‡c tÃ¡i cáº¥u trÃºc hoáº·c Ä‘á»‹nh dáº¡ng dá»¯ liá»‡u theo yÃªu cáº§u cá»§a quÃ¡ trÃ¬nh xá»­ lÃ½ tiáº¿p theo.\\n- **Xá»­ lÃ½**: Sau khi chuyá»ƒn Ä‘á»•i, dá»¯ liá»‡u sáº½ Ä‘Æ°á»£c chuyá»ƒn Ä‘áº¿n má»™t hÃ m Lambda Ä‘á»ƒ xá»­ lÃ½. Äiá»u nÃ y cÃ³ thá»ƒ liÃªn quan Ä‘áº¿n cÃ¡c tÃ­nh toÃ¡n, xÃ¡c thá»±c hoáº·c báº¥t ká»³ logic nghiá»‡p vá»¥ nÃ o.\\n\\n**BÆ°á»›c 2: XÃ¡c Ä‘á»‹nh Cáº¥u trÃºc Quy trÃ¬nh lÃ m viá»‡c**\\nAWS Step Functions sá»­ dá»¥ng má»™t mÃ¡y tráº¡ng thÃ¡i Ä‘á»ƒ xÃ¡c Ä‘á»‹nh quy trÃ¬nh lÃ m viá»‡c. Quy trÃ¬nh lÃ m viá»‡c sáº½ cÃ³ cÃ¡c tráº¡ng thÃ¡i sau:\\n1. **Tráº¡ng thÃ¡i Báº¯t Ä‘áº§u**: Khá»Ÿi táº¡o quy trÃ¬nh lÃ m viá»‡c.\\n2. **Tráº¡ng thÃ¡i Chuyá»ƒn Ä‘á»•i Dá»¯ liá»‡u**: Ãp dá»¥ng máº«u Ã¡nh xáº¡ cho dá»¯ liá»‡u JSON Ä‘áº§u vÃ o.\\n3. **Tráº¡ng thÃ¡i Xá»­ lÃ½ Dá»¯ liá»‡u**: Gá»i hÃ m Lambda vá»›i dá»¯ liá»‡u Ä‘Ã£ chuyá»ƒn Ä‘á»•i.\\n4. **Tráº¡ng thÃ¡i Káº¿t thÃºc**: HoÃ n thÃ nh quy trÃ¬nh lÃ m viá»‡c.\\n\\n**BÆ°á»›c 3: Chá»n ÄÃºng Loáº¡i Tráº¡ng thÃ¡i**\\n- **Tráº¡ng thÃ¡i Chuyá»ƒn Ä‘á»•i Dá»¯ liá»‡u**: ÄÃ¢y cÃ³ thá»ƒ lÃ  tráº¡ng thÃ¡i **Pass** vá»›i **ResultPath** Ã¡p dá»¥ng máº«u Ã¡nh xáº¡.\\n- **Tráº¡ng thÃ¡i Xá»­ lÃ½ Dá»¯ liá»‡u**: ÄÃ¢y sáº½ lÃ  tráº¡ng thÃ¡i **Task** gá»i hÃ m Lambda.\\n- **Tráº¡ng thÃ¡i Báº¯t Ä‘áº§u vÃ  Káº¿t thÃºc**: ÄÃ¢y lÃ  cÃ¡c tráº¡ng thÃ¡i tiÃªu chuáº©n Ä‘á»ƒ báº¯t Ä‘áº§u vÃ  káº¿t thÃºc quy trÃ¬nh lÃ m viá»‡c.\\n\\n**BÆ°á»›c 4: Viáº¿t Máº«u Ãnh xáº¡**\\nMáº«u Ã¡nh xáº¡ sáº½ xÃ¡c Ä‘á»‹nh cÃ¡ch dá»¯ liá»‡u JSON Ä‘áº§u vÃ o Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i. VÃ­ dá»¥, nÃ³ cÃ³ thá»ƒ trÃ­ch xuáº¥t cÃ¡c trÆ°á»ng cá»¥ thá»ƒ, Ä‘á»•i tÃªn chÃºng hoáº·c tÃ¡i cáº¥u trÃºc há»‡ thá»‘ng phÃ¢n cáº¥p dá»¯ liá»‡u. Máº«u cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t báº±ng JSON hoáº·c má»™t Ä‘á»‹nh dáº¡ng Ä‘Æ°á»£c há»— trá»£ khÃ¡c nhÆ° JSONPath.\\n\\n**BÆ°á»›c 5: Cáº¥u hÃ¬nh HÃ m Lambda**\\nÄáº£m báº£o ráº±ng hÃ m Lambda Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘Ãºng Ä‘á»ƒ nháº­n dá»¯ liá»‡u Ä‘Ã£ chuyá»ƒn Ä‘á»•i. Äiá»u nÃ y bao gá»“m viá»‡c thiáº¿t láº­p Ä‘Ãºng vai trÃ² vÃ  quyá»n IAM Ä‘á»ƒ Step Functions cÃ³ thá»ƒ gá»i hÃ m Lambda.\\n\\n**BÆ°á»›c 6: Xá»­ lÃ½ Lá»—i vÃ  Thá»­ láº¡i**\\nÄiá»u quan trá»ng lÃ  pháº£i bao gá»“m xá»­ lÃ½ lá»—i Ä‘á»ƒ quáº£n lÃ½ cÃ¡c lá»—i trong quÃ¡ trÃ¬nh chuyá»ƒn Ä‘á»•i hoáº·c xá»­ lÃ½. Äiá»u nÃ y cÃ³ thá»ƒ liÃªn quan Ä‘áº¿n cÃ¡c cÆ¡ cháº¿ thá»­ láº¡i hoáº·c cÃ¡c tráº¡ng thÃ¡i dá»± phÃ²ng.\\n\\n**BÆ°á»›c 7: XÃ¡c Ä‘á»‹nh MÃ¡y Tráº¡ng thÃ¡i trong JSON**\\nBÃ¢y giá», tÃ´i sáº½ cáº¥u trÃºc mÃ¡y tráº¡ng thÃ¡i trong JSON, káº¿t há»£p táº¥t cáº£ cÃ¡c yáº¿u tá»‘ trÃªn. TÃ´i sáº½ Ä‘áº£m báº£o ráº±ng má»—i tráº¡ng thÃ¡i Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh chÃ­nh xÃ¡c vá»›i loáº¡i cá»§a nÃ³, tráº¡ng thÃ¡i tiáº¿p theo vÃ  báº¥t ká»³ tham sá»‘ cáº§n thiáº¿t nÃ o nhÆ° máº«u Ã¡nh xáº¡ vÃ  ARN hÃ m Lambda.\\n\\n**BÆ°á»›c 8: Xem xÃ©t vÃ  XÃ¡c thá»±c**\\nSau khi phÃ¡c tháº£o JSON, tÃ´i sáº½ xem xÃ©t nÃ³ Ä‘á»ƒ Ä‘áº£m báº£o táº¥t cáº£ cÃº phÃ¡p Ä‘á»u chÃ­nh xÃ¡c, cÃ¡c tráº¡ng thÃ¡i Ä‘Æ°á»£c liÃªn káº¿t Ä‘Ãºng cÃ¡ch vÃ  quy trÃ¬nh lÃ m viá»‡c logic tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i. TÃ´i cÅ©ng sáº½ xÃ¡c thá»±c ráº±ng hÃ m Lambda Ä‘Æ°á»£c tham chiáº¿u chÃ­nh xÃ¡c vÃ  máº«u Ã¡nh xáº¡ chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u chÃ­nh xÃ¡c nhÆ° dá»± Ä‘á»‹nh.\\n\\n**CÃ¡c ThÃ¡ch thá»©c vÃ  CÃ¢n nháº¯c Tiá»m nÄƒng**\\n- **XÃ¡c thá»±c Dá»¯ liá»‡u**: Äáº£m báº£o ráº±ng dá»¯ liá»‡u Ä‘áº§u vÃ o tuÃ¢n thá»§ Ä‘á»‹nh dáº¡ng dá»± kiáº¿n Ä‘á»ƒ ngÄƒn ngá»«a lá»—i trong quÃ¡ trÃ¬nh chuyá»ƒn Ä‘á»•i.\\n- **Giá»›i háº¡n HÃ m Lambda**: LÆ°u Ã½ Ä‘áº¿n thá»i gian thá»±c thi vÃ  giá»›i háº¡n bá»™ nhá»› cá»§a hÃ m Lambda Ä‘á»ƒ xá»­ lÃ½ hiá»‡u quáº£.\\n- **Xá»­ lÃ½ Lá»—i**: Triá»ƒn khai xá»­ lÃ½ lá»—i máº¡nh máº½ Ä‘á»ƒ quáº£n lÃ½ má»™t cÃ¡ch duyÃªn dÃ¡ng báº¥t ká»³ lá»—i nÃ o mÃ  khÃ´ng lÃ m cho toÃ n bá»™ quy trÃ¬nh lÃ m viá»‡c bá»‹ lá»—i báº¥t ngá».\\n- **Báº£o máº­t**: Äáº£m báº£o ráº±ng Step Functions vÃ  hÃ m Lambda cÃ³ cÃ¡c Ä‘áº·c quyá»n tá»‘i thiá»ƒu cáº§n thiáº¿t Ä‘á»ƒ thá»±c hiá»‡n cÃ¡c tÃ¡c vá»¥ cá»§a chÃºng, tuÃ¢n thá»§ nguyÃªn táº¯c Ä‘áº·c quyá»n tá»‘i thiá»ƒu.\\n\\nBáº±ng cÃ¡ch láº­p káº¿ hoáº¡ch tá»‰ má»‰ cho tá»«ng thÃ nh pháº§n vÃ  dá»± Ä‘oÃ¡n cÃ¡c váº¥n Ä‘á» tiá»m áº©n, tÃ´i cÃ³ thá»ƒ táº¡o ra má»™t quy trÃ¬nh lÃ m viá»‡c AWS Step Functions Ä‘Ã¡ng tin cáº­y vÃ  hiá»‡u quáº£ Ä‘Ã¡p á»©ng cÃ¡c yÃªu cáº§u.</reasoning>\\n\\n<answer>{\\n  \"StartAt\": \"Transform Data\",\\n  \"States\": {\\n    \"Transform Data\": {\\n      \"Type\": \"Pass\",\\n      \"Result\": {\\n        \"inputData\": {\\n          \"name\": \"$.input.name\",\\n          \"age\": \"$.input.age\",\\n          \"email\": \"$.input.email\"\\n        }\\n      },\\n      \"ResultPath\": \"$.transformedData\",\\n      \"Next\": \"Process Data\"\\n    },\\n    \"Process Data\": {\\n      \"Type\": \"Task\",\\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:ProcessDataFunction\",\\n      \"InputPath\": \"$.transformedData\",\\n      \"ResultPath\": \"$.processedData\",\\n      \"Next\": \"End State\"\\n    },\\n    \"End State\": {\\n      \"Type\": \"Succeed\"\\n    }\\n  }\\n}</answer><|eot_id|>'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "_rD6fl8EUxnG",
        "outputId": "073b8bc5-20e2-4eae-cd51-42d84a791a66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'                                                                                                                                                                                                                                                 \\n\\n<reasoning>ÄÆ°á»£c thÃ´i, tÃ´i cáº§n tÃ¬m cÃ¡ch sá»­a Ä‘á»•i code hiá»‡n cÃ³ Ä‘á»ƒ quáº£n lÃ½ máº£ng cÃ¡c hÃ nh tinh trong Há»‡ Máº·t Trá»i cá»§a chÃºng ta Ä‘á»ƒ thÃªm má»™t hÃ nh tinh má»›i. HÃ nh tinh má»›i pháº£i tuÃ¢n theo quy Æ°á»›c Ä‘áº·t tÃªn cá»§a tháº§n thoáº¡i cá»• Ä‘iá»ƒn vÃ  pháº£i Ä‘Æ°á»£c Ä‘á»‹nh vá»‹ sao cho nÃ³ khÃ´ng gáº§n Máº·t Trá»i hÆ¡n Sao Thá»§y hoáº·c xa hÆ¡n Sao DiÃªm VÆ°Æ¡ng. ChÃºng ta hÃ£y chia nhá» váº¥n Ä‘á» nÃ y tá»«ng bÆ°á»›c.\\n\\nÄáº§u tiÃªn, tÃ´i nÃªn xem xÃ©t cáº¥u trÃºc hiá»‡n táº¡i cá»§a máº£ng hÃ nh tinh. NÃ³ cÃ³ kháº£ nÄƒng lÃ  má»™t máº£ng cÃ¡c Ä‘á»‘i tÆ°á»£ng, trong Ä‘Ã³ má»—i Ä‘á»‘i tÆ°á»£ng Ä‘áº¡i diá»‡n cho má»™t hÃ nh tinh vá»›i cÃ¡c thuá»™c tÃ­nh nhÆ° tÃªn, khoáº£ng cÃ¡ch tá»« Máº·t Trá»i vÃ  cÃ³ thá»ƒ cÃ¡c thuá»™c tÃ­nh khÃ¡c. Äá»ƒ thÃªm má»™t hÃ nh tinh má»›i, tÃ´i sáº½ cáº§n táº¡o má»™t Ä‘á»‘i tÆ°á»£ng má»›i vá»›i cÃ¡c thuá»™c tÃ­nh nÃ y.\\n\\nTiáº¿p theo, quy Æ°á»›c Ä‘áº·t tÃªn yÃªu cáº§u tÃªn cá»§a hÃ nh tinh má»›i pháº£i báº¯t nguá»“n tá»« tháº§n thoáº¡i cá»• Ä‘iá»ƒn. TÃ´i nÃªn nghÄ© Ä‘áº¿n nhá»¯ng cÃ¡i tÃªn tá»« tháº§n thoáº¡i Hy Láº¡p hoáº·c La MÃ£ chÆ°a Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi cÃ¡c hÃ nh tinh hiá»‡n cÃ³. CÃ³ láº½ má»™t cÃ¡i gÃ¬ Ä‘Ã³ nhÆ° \"Lysithea\" hoáº·c \"Circe\". TÃ´i cáº§n Ä‘áº£m báº£o ráº±ng tÃªn Ä‘Ã£ chá»n phÃ¹ há»£p vá»›i quy táº¯c Ä‘áº·t tÃªn hiá»‡n cÃ³ vÃ  chÆ°a Ä‘Æ°á»£c sá»­ dá»¥ng.\\n\\nBÃ¢y giá», vá» khoáº£ng cÃ¡ch tá»« Máº·t Trá»i, hÃ nh tinh má»›i khÃ´ng Ä‘Æ°á»£c gáº§n hÆ¡n Sao Thá»§y hoáº·c xa hÆ¡n Sao DiÃªm VÆ°Æ¡ng. Sao Thá»§y cÃ¡ch Máº·t Trá»i khoáº£ng 58 triá»‡u km vÃ  Sao DiÃªm VÆ°Æ¡ng cÃ¡ch khoáº£ng 5,9 tá»· km. VÃ¬ váº­y, khoáº£ng cÃ¡ch cá»§a hÃ nh tinh má»›i pháº£i náº±m trong pháº¡m vi nÃ y. TÃ´i sáº½ cáº§n quyáº¿t Ä‘á»‹nh chÃ­nh xÃ¡c vá»‹ trÃ­ Ä‘áº·t nÃ³. CÃ³ láº½ Ä‘Ã¢u Ä‘Ã³ giá»¯a TrÃ¡i Äáº¥t vÃ  Sao Há»a Ä‘á»ƒ duy trÃ¬ sá»± phÃ¢n bá»‘ cÃ¢n báº±ng.\\n\\nTÃ´i cÅ©ng nÃªn xem xÃ©t nhá»¯ng tÃ¡c Ä‘á»™ng cá»§a viá»‡c thÃªm má»™t hÃ nh tinh má»›i. Äiá»u nÃ y cÃ³ áº£nh hÆ°á»Ÿng Ä‘áº¿n cÃ¡c pháº§n khÃ¡c cá»§a code, cháº³ng háº¡n nhÆ° cÃ¡c phÃ©p tÃ­nh liÃªn quan Ä‘áº¿n chu ká»³ quá»¹ Ä‘áº¡o hoáº·c tÆ°Æ¡ng tÃ¡c háº¥p dáº«n khÃ´ng? Náº¿u váº­y, tÃ´i cÅ©ng cÃ³ thá»ƒ cáº§n cáº­p nháº­t cÃ¡c pháº§n Ä‘Ã³ Ä‘á»ƒ tÃ­nh Ä‘áº¿n sá»± hiá»‡n diá»‡n cá»§a hÃ nh tinh má»›i.\\n\\nNgoÃ i ra, tÃ´i cáº§n Ä‘áº£m báº£o ráº±ng code váº«n hiá»‡u quáº£ vÃ  dá»… báº£o trÃ¬. CÃ³ láº½ viá»‡c triá»ƒn khai má»™t hÃ m Ä‘á»ƒ thÃªm cÃ¡c hÃ nh tinh má»™t cÃ¡ch linh hoáº¡t sáº½ cÃ³ lá»£i, cho phÃ©p cÃ¡c láº§n thÃªm trong tÆ°Æ¡ng lai mÃ  khÃ´ng cáº§n sá»­a Ä‘á»•i cáº¥u trÃºc cá»‘t lÃµi. HÃ m nÃ y cÃ³ thá»ƒ xÃ¡c thá»±c tÃªn cá»§a hÃ nh tinh má»›i so vá»›i cÃ¡c tÃªn hiá»‡n cÃ³ vÃ  kiá»ƒm tra xem khoáº£ng cÃ¡ch cá»§a nÃ³ cÃ³ náº±m trong pháº¡m vi quy Ä‘á»‹nh hay khÃ´ng.\\n\\nXá»­ lÃ½ lá»—i lÃ  má»™t khÃ­a cáº¡nh khÃ¡c cáº§n xem xÃ©t. Äiá»u gÃ¬ sáº½ xáº£y ra náº¿u ai Ä‘Ã³ cá»‘ gáº¯ng thÃªm má»™t hÃ nh tinh cÃ³ tÃªn Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng hoáº·c cÃ³ khoáº£ng cÃ¡ch khÃ´ng há»£p lá»‡? Code nÃªn xá»­ lÃ½ cÃ¡c trÆ°á»ng há»£p nÃ y má»™t cÃ¡ch uyá»ƒn chuyá»ƒn, cÃ³ thá»ƒ báº±ng cÃ¡ch Ä‘Æ°a ra lá»—i hoáº·c tráº£ vá» má»™t thÃ´ng bÃ¡o cho biáº¿t váº¥n Ä‘á».\\n\\nKiá»ƒm thá»­ lÃ  ráº¥t quan trá»ng. Sau khi thÃªm hÃ nh tinh má»›i, tÃ´i nÃªn cháº¡y thá»­ nghiá»‡m Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng máº£ng Ä‘Æ°á»£c cáº­p nháº­t chÃ­nh xÃ¡c vÃ  táº¥t cáº£ cÃ¡c chá»©c nÄƒng liÃªn quan váº«n hoáº¡t Ä‘á»™ng nhÆ° mong Ä‘á»£i. Äiá»u nÃ y bao gá»“m viá»‡c xÃ¡c minh ráº±ng cÃ¡c thuá»™c tÃ­nh cá»§a hÃ nh tinh má»›i Ä‘Æ°á»£c gÃ¡n chÃ­nh xÃ¡c vÃ  nÃ³ Ä‘Æ°á»£c tÃ­ch há»£p Ä‘Ãºng vÃ o há»‡ thá»‘ng.\\n\\nTÃ i liá»‡u cÅ©ng ráº¥t quan trá»ng. Viá»‡c cáº­p nháº­t cÃ¡c nháº­n xÃ©t vÃ  báº¥t ká»³ tÃ i liá»‡u liÃªn quan nÃ o Ä‘á»ƒ pháº£n Ã¡nh viá»‡c bá»• sung hÃ nh tinh má»›i sáº½ giÃºp cÃ¡c nhÃ  phÃ¡t triá»ƒn trong tÆ°Æ¡ng lai hiá»ƒu Ä‘Æ°á»£c nhá»¯ng thay Ä‘á»•i Ä‘Ã£ thá»±c hiá»‡n. Äiá»u nÃ y bao gá»“m viá»‡c giáº£i thÃ­ch quy Æ°á»›c Ä‘áº·t tÃªn vÃ  cÃ¡c rÃ ng buá»™c vá» khoáº£ng cÃ¡ch.\\n\\nCuá»‘i cÃ¹ng, tÃ´i nÃªn nghÄ© vá» nhá»¯ng tÃ¡c Ä‘á»™ng cá»§a giao diá»‡n ngÆ°á»i dÃ¹ng náº¿u máº£ng hÃ nh tinh Ä‘Æ°á»£c sá»­ dá»¥ng trong má»™t thÃ nh pháº§n UI. HÃ nh tinh má»›i pháº£i Ä‘Æ°á»£c hiá»ƒn thá»‹ chÃ­nh xÃ¡c, vá»›i tÃªn vÃ  khoáº£ng cÃ¡ch cá»§a nÃ³ Ä‘Æ°á»£c thá»ƒ hiá»‡n má»™t cÃ¡ch chÃ­nh xÃ¡c. Äiá»u nÃ y cÃ³ thá»ƒ liÃªn quan Ä‘áº¿n viá»‡c cáº­p nháº­t cÃ¡c máº«u hoáº·c cháº¿ Ä‘á»™ xem Ä‘á»ƒ phÃ¹ há»£p vá»›i dá»¯ liá»‡u bá»• sung.\\n\\nTÃ³m láº¡i, viá»‡c thÃªm má»™t hÃ nh tinh má»›i bao gá»“m viá»‡c táº¡o má»™t Ä‘á»‘i tÆ°á»£ng má»›i vá»›i má»™t cÃ¡i tÃªn phÃ¹ há»£p vá»›i tháº§n thoáº¡i, Ä‘áº£m báº£o khoáº£ng cÃ¡ch cá»§a nÃ³ náº±m trong pháº¡m vi Sao Thá»§y-Sao DiÃªm VÆ°Æ¡ng, cáº­p nháº­t máº£ng, xá»­ lÃ½ cÃ¡c lá»—i tiá»m áº©n, kiá»ƒm tra cÃ¡c thay Ä‘á»•i, cáº­p nháº­t tÃ i liá»‡u vÃ  xem xÃ©t cÃ¡c tÃ¡c Ä‘á»™ng cá»§a UI. Báº±ng cÃ¡ch lÃ m theo cÃ¡c bÆ°á»›c nÃ y má»™t cÃ¡ch cÃ³ phÆ°Æ¡ng phÃ¡p, tÃ´i cÃ³ thá»ƒ tÃ­ch há»£p thÃ nh cÃ´ng hÃ nh tinh má»›i vÃ o há»‡ thá»‘ng hiá»‡n cÃ³.</reasoning>\\n\\n<answer>```javascript\\n// Máº£ng cÃ¡c hÃ nh tinh hiá»‡n cÃ³\\nlet planets = [\\n    { name: \\'Mercury\\', distanceFromSun: 58 }, // tÃ­nh báº±ng triá»‡u km\\n    { name: \\'Venus\\', distanceFromSun: 108 },\\n    { name: \\'Earth\\', distanceFromSun: 149.6 },\\n    { name: \\'Mars\\', distanceFromSun: 227.9 },\\n    { name: \\'Jupiter\\', distanceFromSun: 778.3 },\\n    { name: \\'Saturn\\', distanceFromSun: 1427 },\\n    { name: \\'Uranus\\', distanceFromSun: 2871 },\\n    { name: \\'Neptune\\', distanceFromSun: 4497 },\\n    { name: \\'Pluto\\', distanceFromSun: 5900 }\\n];\\n\\n// HÃ m thÃªm má»™t hÃ nh tinh má»›i\\nfunction addPlanet(newPlanet) {\\n    // XÃ¡c thá»±c tÃªn chÆ°a Ä‘Æ°á»£c sá»­ dá»¥ng\\n    const nameExists = planets.some(planet => planet.name === newPlanet.name);\\n    if (nameExists) {\\n        console.error(`HÃ nh tinh tÃªn ${newPlanet.name} Ä‘Ã£ tá»“n táº¡i.`);\\n        return;\\n    }\\n\\n    // XÃ¡c thá»±c khoáº£ng cÃ¡ch náº±m trong pháº¡m vi cá»§a Sao Thá»§y vÃ  Sao DiÃªm VÆ°Æ¡ng\\n    if (newPlanet.distanceFromSun < planets[0].distanceFromSun || newPlanet.distanceFromSun > planets[planets.length - 1].distanceFromSun) {\\n        console.error(`HÃ nh tinh ${newPlanet.name} náº±m ngoÃ i pháº¡m vi khoáº£ng cÃ¡ch cho phÃ©p.`);\\n        return;\\n    }\\n\\n    // ThÃªm hÃ nh tinh má»›i vÃ o máº£ng\\n    planets.push(newPlanet);\\n    console.log(`ÄÃ£ thÃªm thÃ nh cÃ´ng hÃ nh tinh ${newPlanet.name}.`);\\n}\\n\\n// Chi tiáº¿t hÃ nh tinh má»›i\\nconst newPlanet = {\\n    name: \\'Lysithea\\', // Báº¯t nguá»“n tá»« tháº§n thoáº¡i cá»• Ä‘iá»ƒn\\n    distanceFromSun: 300 // tÃ­nh báº±ng triá»‡u km\\n};\\n\\n// ThÃªm hÃ nh tinh má»›i\\naddPlanet(newPlanet);\\n\\n// Máº£ng cÃ¡c hÃ nh tinh Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t\\nconsole.log(planets);\\n```</answer><|eot_id|>'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "space = tokenizer(\" \", add_special_tokens=False).input_ids[0]\n",
        "tokenizer.decode(\n",
        "    [space if x == -100 else x for x in trainer.train_dataset[6][\"labels\"]]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3enWUM0jV-jV"
      },
      "source": [
        "We can see the System and Instruction prompts are successfully masked!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "a2a7d5f0-4f83-4a63-ce92-ec2f384af156"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = NVIDIA GeForce RTX 4060. Max memory = 7.739 GB.\n",
            "2.301 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "5d386664-e08f-47ab-a0d7-44adaccb2f28"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 5,372 | Num Epochs = 2\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 3\n",
            "\\        /    Total batch size = 6 | Total steps = 1,790\n",
            " \"-____-\"     Number of trainable parameters = 24,313,856\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='1790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   2/1790 : < :, Epoch 0.00/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/project/fine_tuning_model/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:361\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 361\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
            "File \u001b[0;32m~/Documents/project/fine_tuning_model/.venv/lib/python3.12/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m<string>:329\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "File \u001b[0;32m<string>:73\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n",
            "File \u001b[0;32m~/Documents/project/fine_tuning_model/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:2329\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2329\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/project/fine_tuning_model/.venv/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/project/fine_tuning_model/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/project/fine_tuning_model/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "fcbecf7f-b8a1-45d5-f415-eec2bdf96576"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'trainer_stats' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m used_percentage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(used_memory \u001b[38;5;241m/\u001b[39m max_memory \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m lora_percentage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(used_memory_for_lora \u001b[38;5;241m/\u001b[39m max_memory \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtrainer_stats\u001b[49m\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_runtime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds used for training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(trainer_stats\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_runtime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes used for training.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPeak reserved memory = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mused_memory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GB.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer_stats' is not defined"
          ]
        }
      ],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "\n",
        "### Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "2f600133-5925-4348-f39d-f57a2efbd17c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTiáº¿p tá»¥c chuá»—i fibonacci: 1, 1, 2, 3, 5, 8,<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nTáº¡o hÃ m fibonacci láº¥y tá»« biáº¿n fibonacci cÃ³ giÃ¡ trá»‹ ban Ä‘áº§u 0,1:\\n\\nTiáº¿p tá»¥c chuá»—i fibonacci: 1, 1, 2, 3, 5, 8, 13, 21, 34,...\\n\\nTiáº¿p tá»¥c chuá»—i fibonacci: 1, 1, 2, 3, 5, 8, 13, 21,...\\n\\nTiáº¿p tá»¥c chuá»—i fibonacci: 1, 1, 2, 3, 5, 8,...\\n\\nTiáº¿p tá»¥c chuá»—i fibonacci: 1, 1, 2, 3,...\\n\\nTi']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"llama-3.1\",\n",
        ")\n",
        "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Tiáº¿p tá»¥c chuá»—i fibonacci: 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,  # Must add for generation\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs, max_new_tokens=128, use_cache=True, temperature=1.5, min_p=0.1\n",
        ")\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        "You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "e2pEuRb1r2Vg",
        "outputId": "ffee7558-1224-46ba-aaf0-6852c64f91e8"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": \"Tiáº¿p tá»¥c chuá»—i Fibonacci: 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,  # Must add for generation\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "_ = model.generate(\n",
        "    input_ids=inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=128,\n",
        "    use_cache=True,\n",
        "    temperature=1.5,\n",
        "    min_p=0.1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "\n",
        "### Saving, loading finetuned models\n",
        "\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "2b97c884-8e78-44fb-8726-f95e0634058b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "MKX_XKs_BNZR",
        "outputId": "7b1425af-cd0e-4f6a-dffe-e0f342019aa7"
      },
      "outputs": [],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"lora_model\",  # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length=max_seq_length,\n",
        "        dtype=dtype,\n",
        "        load_in_4bit=load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": \"Tiáº¿p tá»¥c chuá»—i fibonacci: 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,  # Must add for generation\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "_ = model.generate(\n",
        "    input_ids=inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=4000,\n",
        "    use_cache=True,\n",
        "    temperature=0.5,\n",
        "    min_p=0.1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQMjaNrjsU5_"
      },
      "source": [
        "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFfaXG0WsQuE",
        "outputId": "6d08819b-1169-42b6-e008-8e333998def5"
      },
      "outputs": [],
      "source": [
        "if True:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"lora_model\",  # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit=load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43qBkEbXzCaN"
      },
      "source": [
        "# Hoáº·c dÃ¹ng cÃ¡ch dÆ°á»›i\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOheGXS0zAf5",
        "outputId": "e6af13ef-c112-4391-cfe1-6578d4a6c30e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load model vÃ  tokenizer\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    \"load_models\",\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")\n",
        "\n",
        "# Merge LoRA vÃ o base model\n",
        "merged_model = model.merge_and_unload()  # â† BÆ°á»›c quan trá»ng!\n",
        "\n",
        "# LÆ°u model Ä‘Ã£ merge\n",
        "merged_model.save_pretrained(\n",
        "    \"output/merged_model\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=False,\n",
        ")\n",
        "tokenizer.save_pretrained(\"output/merged_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if True:\n",
        "    model.save_pretrained(\n",
        "        save_directory=\"ChunThinker/lora_model_vllm\",\n",
        "        tokenizer=tokenizer,  # LÆ°u tokenizer riÃªng\n",
        "        save_method=\"merged_16bit\",  # Merge adapter vÃ o base model\n",
        "        safe_serialization=True,  # Báº¯t buá»™c Ä‘á»ƒ trÃ¡nh lá»—i\n",
        "    )\n",
        "if False:\n",
        "    model.push_to_hub_merged(\n",
        "        \"hf/model\", tokenizer, save_method=\"merged_16bit\", token=\"\"\n",
        "    )\n",
        "\n",
        "# Merge to 4bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\n",
        "        \"model\",\n",
        "        tokenizer,\n",
        "        save_method=\"merged_4bit\",\n",
        "    )\n",
        "if False:\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_4bit\", token=\"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained_merged(\n",
        "        \"model\",\n",
        "        tokenizer,\n",
        "        save_method=\"lora\",\n",
        "    )\n",
        "if False:\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"lora\", token=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "\n",
        "- `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "- `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "- `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\n",
        "        \"model\",\n",
        "        tokenizer,\n",
        "    )\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, token=\"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if True:\n",
        "    model.save_pretrained(\n",
        "        \"output/model/gguf\", tokenizer, quantization_method=\"f16\"\n",
        "    )\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method=\"f16\", token=\"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\")\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", tokenizer, quantization_method=\"q4_k_m\", token=\"\"\n",
        "    )\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\",  # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method=[\n",
        "            \"q4_k_m\",\n",
        "            \"q8_0\",\n",
        "            \"q5_k_m\",\n",
        "        ],\n",
        "        token=\"\",  # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05361a49bec247a989b92d0273f8a8a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_261f2d39bc1247b1b7ec557c92626f44",
            "max": 11372,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a8544ba4489494097d446606349043a",
            "value": 11372
          }
        },
        "070ca8d4788040439d70b289123ed7f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "12184c89275c42a59f3c5f7a512cf8e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14c6d25d2c6a433ca772fbf8c95061d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14fcca8d922a450c8afcab53e5aa65e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14c6d25d2c6a433ca772fbf8c95061d8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8b6a38b01cd44c73859da2e459e8a80f",
            "value": "Map:â€‡100%"
          }
        },
        "1a92e911567146e5908e7e680ecd4905": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37a5403876f8466ba0cf676ca1a3f345",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b8cb050625fe44f59cc75fb4669ee287",
            "value": "â€‡11372/11372â€‡[00:14&lt;00:00,â€‡720.19â€‡examples/s]"
          }
        },
        "248d471ae26842e7947a1c4087f8097e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "261f2d39bc1247b1b7ec557c92626f44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "270314604ce74721b857c5204d41b283": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29f24afedecc43db95ac4e7ab1fa01e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dd1efc8384f4356ade13143f9774bdd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8dc92c23eff34236aedc83724db607b7",
            "value": "â€‡11372/11372â€‡[00:23&lt;00:00,â€‡633.90â€‡examples/s]"
          }
        },
        "326c8c02704742aba6eb9906b6ab565d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7720d0627c7e4770bb8417ca3d53e82c",
              "IPY_MODEL_05361a49bec247a989b92d0273f8a8a6",
              "IPY_MODEL_fda2b821efb347258145ac593cf49c0d"
            ],
            "layout": "IPY_MODEL_a830b7f0feda4748978b95024204e1bc"
          }
        },
        "34c9e9cfbf104c90a8d0b8ec30021edd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37a5403876f8466ba0cf676ca1a3f345": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a7731db739e4674a93db274fb135d7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3db45cd840274a8f8b4835d4b594839d",
            "max": 11372,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5bc57b3afb94473a994077567332651",
            "value": 11372
          }
        },
        "3cc9bdfb85704b17a64f7097ebf4a44b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9705c35042184d2db14ebae9d9f1ecfa",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_270314604ce74721b857c5204d41b283",
            "value": "Tokenizingâ€‡trainâ€‡datasetâ€‡(num_proc=2):â€‡100%"
          }
        },
        "3db45cd840274a8f8b4835d4b594839d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dd1efc8384f4356ade13143f9774bdd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40edbd257909455b95f13c75ccec8661": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "467ba48099b0486798322ceb4b206bf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df7d41a027c8446ebb11bd0210c5c103",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c484ba18d3134eb8815498b2328717bb",
            "value": "Map:â€‡100%"
          }
        },
        "4d14e81bc0c642ad833e61f764f521e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee420abea7814a7c874cc4d30d53778f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_34c9e9cfbf104c90a8d0b8ec30021edd",
            "value": "â€‡11372/11372â€‡[00:03&lt;00:00,â€‡7321.18â€‡examples/s]"
          }
        },
        "51fc4cde067746cda57fbc2b64e02d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a8544ba4489494097d446606349043a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e5ed953d2a741cbb9ba6d954a1e7c18": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7720d0627c7e4770bb8417ca3d53e82c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9884af3cc32647acaf3b9de913d4d21d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_51fc4cde067746cda57fbc2b64e02d40",
            "value": "Tokenizingâ€‡trainâ€‡datasetâ€‡(num_proc=2):â€‡100%"
          }
        },
        "796191af9a0d4bab9377d13a05da4b5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7780eaba753460eb5d03a48cf0c371a",
            "max": 11372,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_070ca8d4788040439d70b289123ed7f5",
            "value": 11372
          }
        },
        "7f69c2f0ad70491b82ed1b550c2741b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_467ba48099b0486798322ceb4b206bf5",
              "IPY_MODEL_3a7731db739e4674a93db274fb135d7e",
              "IPY_MODEL_1a92e911567146e5908e7e680ecd4905"
            ],
            "layout": "IPY_MODEL_c9f860dd5ef24a42988a1adb79a1a30b"
          }
        },
        "8b6a38b01cd44c73859da2e459e8a80f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8dc92c23eff34236aedc83724db607b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e4c415dfe09453bbe50816fc815b1a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40edbd257909455b95f13c75ccec8661",
            "max": 11372,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0072ad5352348c3a551f1c6818445d4",
            "value": 11372
          }
        },
        "9705c35042184d2db14ebae9d9f1ecfa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9884af3cc32647acaf3b9de913d4d21d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a211854308d4cf896022aead86a6fc6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0072ad5352348c3a551f1c6818445d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3ec2615a9d343329f17a4a2278a8a14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3cc9bdfb85704b17a64f7097ebf4a44b",
              "IPY_MODEL_e3a9a42cae3d419e95ffedc046a9887c",
              "IPY_MODEL_29f24afedecc43db95ac4e7ab1fa01e0"
            ],
            "layout": "IPY_MODEL_be00b0e96faf46098160b2dd33b448cc"
          }
        },
        "a5bc57b3afb94473a994077567332651": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7780eaba753460eb5d03a48cf0c371a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a830b7f0feda4748978b95024204e1bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8cb050625fe44f59cc75fb4669ee287": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb2e1e90765e4ce1a02cf7e8a9f6d881": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be00b0e96faf46098160b2dd33b448cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2c710d06dd04428a8595296c52828bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12184c89275c42a59f3c5f7a512cf8e6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c65192e89e6046299174394d62c31002",
            "value": "Applyingâ€‡chatâ€‡templateâ€‡toâ€‡trainâ€‡datasetâ€‡(num_proc=2):â€‡100%"
          }
        },
        "c484ba18d3134eb8815498b2328717bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c65192e89e6046299174394d62c31002": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7e834a42885469cb4e291a0c78ede14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14fcca8d922a450c8afcab53e5aa65e1",
              "IPY_MODEL_796191af9a0d4bab9377d13a05da4b5f",
              "IPY_MODEL_e1d893dc93374ff389b9283019d3bbea"
            ],
            "layout": "IPY_MODEL_248d471ae26842e7947a1c4087f8097e"
          }
        },
        "c9f860dd5ef24a42988a1adb79a1a30b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd1085f167374f8f96a4fd61feb125c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d664254d90224b0d9738d7fc92dac1c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbd1c96a6c4a4b3a81457436407e75ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df7d41a027c8446ebb11bd0210c5c103": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1d893dc93374ff389b9283019d3bbea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb2e1e90765e4ce1a02cf7e8a9f6d881",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e84444abd7e843328a762cade219777d",
            "value": "â€‡11372/11372â€‡[00:02&lt;00:00,â€‡6281.70â€‡examples/s]"
          }
        },
        "e3a9a42cae3d419e95ffedc046a9887c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d664254d90224b0d9738d7fc92dac1c9",
            "max": 11372,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd1085f167374f8f96a4fd61feb125c9",
            "value": 11372
          }
        },
        "e84444abd7e843328a762cade219777d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee420abea7814a7c874cc4d30d53778f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f66ceb5f96fe4ae38193a4a6f9a4fd60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2c710d06dd04428a8595296c52828bb",
              "IPY_MODEL_8e4c415dfe09453bbe50816fc815b1a2",
              "IPY_MODEL_4d14e81bc0c642ad833e61f764f521e3"
            ],
            "layout": "IPY_MODEL_6e5ed953d2a741cbb9ba6d954a1e7c18"
          }
        },
        "fda2b821efb347258145ac593cf49c0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a211854308d4cf896022aead86a6fc6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dbd1c96a6c4a4b3a81457436407e75ec",
            "value": "â€‡11372/11372â€‡[00:59&lt;00:00,â€‡188.99â€‡examples/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
